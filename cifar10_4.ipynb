{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1024 13:34:46.843391 22000 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# from keras.models import load_model\n",
    "from datetime import datetime\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfds.load(name='cifar10:3.*.*', split='train[:80%]', as_supervised=True)\n",
    "valid_data = tfds.load(name='cifar10:3.*.*', split='train[80%:]', as_supervised=True)\n",
    "test_data = tfds.load(name='cifar10:3.*.*', split='test', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 30,890\n",
      "Trainable params: 30,762\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),  # Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나이다\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나이다\n",
    "    tf.keras.layers.MaxPooling2D(),    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "train_data = train_data.map(scale).shuffle(50000).batch(64) # \n",
    "valid_data = valid_data.map(scale).batch(64)\n",
    "test_data = test_data.map(scale).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(   \n",
    "    'checkpoints/{}.hdf5'.format(dt),\n",
    "    monitor='val_acc',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_start(self, epoch, logs=None):\n",
    "        print('Epoch {} begins at {}: {}'.format(epoch, datetime.now().time(), logs))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {} ends at {}: {}'.format(epoch, datetime.now().time(), logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ends at 13:37:29.704781: {'loss': 1.6441709135055542, 'acc': 0.458725, 'val_loss': 1.385859111691736, 'val_acc': 0.5181}\n",
      "Epoch 1 ends at 13:39:13.295584: {'loss': 1.2065217227935792, 'acc': 0.583425, 'val_loss': 1.4675401604858933, 'val_acc': 0.5116}\n",
      "Epoch 2 ends at 13:41:04.977734: {'loss': 1.0747028533935548, 'acc': 0.6258, 'val_loss': 1.1032659381058565, 'val_acc': 0.6153}\n",
      "Epoch 3 ends at 13:42:57.544480: {'loss': 1.0071527797698974, 'acc': 0.647075, 'val_loss': 1.1248166731968048, 'val_acc': 0.6146}\n",
      "Epoch 4 ends at 13:44:39.334103: {'loss': 0.9662142958641052, 'acc': 0.6618, 'val_loss': 0.9696164579148505, 'val_acc': 0.6648}\n",
      "Epoch 5 ends at 13:46:20.134371: {'loss': 0.9272185616493225, 'acc': 0.674825, 'val_loss': 1.1348700648660113, 'val_acc': 0.619}\n",
      "Epoch 6 ends at 13:48:05.656005: {'loss': 0.9022877049446106, 'acc': 0.68645, 'val_loss': 0.9782326680839442, 'val_acc': 0.6628}\n",
      "Epoch 7 ends at 13:49:48.526734: {'loss': 0.8827059274673462, 'acc': 0.693175, 'val_loss': 0.9993097975755193, 'val_acc': 0.659}\n",
      "Epoch 8 ends at 13:51:28.936050: {'loss': 0.8612223142623902, 'acc': 0.698175, 'val_loss': 1.1654077851848237, 'val_acc': 0.6124}\n",
      "Epoch 9 ends at 13:53:15.765185: {'loss': 0.8419998167991638, 'acc': 0.7064, 'val_loss': 0.9704842328265973, 'val_acc': 0.6656}\n",
      "Epoch 10 ends at 13:54:55.689830: {'loss': 0.8318527528762817, 'acc': 0.707925, 'val_loss': 0.9558574716756298, 'val_acc': 0.6731}\n",
      "Epoch 11 ends at 13:56:35.639376: {'loss': 0.8158059610843659, 'acc': 0.715575, 'val_loss': 1.0852821856547312, 'val_acc': 0.635}\n",
      "Epoch 12 ends at 13:58:16.379804: {'loss': 0.8049281812191009, 'acc': 0.71755, 'val_loss': 0.8613618319960916, 'val_acc': 0.7014}\n",
      "Epoch 13 ends at 13:59:56.505877: {'loss': 0.7952092049598694, 'acc': 0.724475, 'val_loss': 0.8974609135822126, 'val_acc': 0.6919}\n",
      "Epoch 14 ends at 14:01:37.434801: {'loss': 0.7868096892833709, 'acc': 0.7237, 'val_loss': 0.8599600831794131, 'val_acc': 0.7046}\n",
      "Epoch 15 ends at 14:03:17.845113: {'loss': 0.7743231106758117, 'acc': 0.72915, 'val_loss': 0.9354904643289602, 'val_acc': 0.6804}\n",
      "Epoch 16 ends at 14:04:58.705222: {'loss': 0.7620008895397187, 'acc': 0.73265, 'val_loss': 0.9248692269917507, 'val_acc': 0.6876}\n",
      "Epoch 17 ends at 14:06:42.852797: {'loss': 0.7535550850868226, 'acc': 0.738275, 'val_loss': 0.9158266349962563, 'val_acc': 0.6888}\n",
      "Epoch 18 ends at 14:08:25.549988: {'loss': 0.7493055509567261, 'acc': 0.7365, 'val_loss': 0.9986962007868821, 'val_acc': 0.6698}\n",
      "Epoch 19 ends at 14:10:06.818004: {'loss': 0.7401250952720642, 'acc': 0.74015, 'val_loss': 0.9081321408034889, 'val_acc': 0.6948}\n",
      "Epoch 20 ends at 14:11:47.863617: {'loss': 0.7341971611976623, 'acc': 0.74135, 'val_loss': 0.9528712983344011, 'val_acc': 0.6835}\n",
      "Epoch 21 ends at 14:13:29.778903: {'loss': 0.7311971059322357, 'acc': 0.745975, 'val_loss': 0.8333404348914031, 'val_acc': 0.7147}\n",
      "Epoch 22 ends at 14:15:11.390001: {'loss': 0.7247919026374817, 'acc': 0.745575, 'val_loss': 0.8862022009624797, 'val_acc': 0.6999}\n",
      "Epoch 23 ends at 14:16:53.778020: {'loss': 0.7205315533161163, 'acc': 0.747125, 'val_loss': 0.8852363270559128, 'val_acc': 0.7004}\n",
      "Epoch 24 ends at 14:18:35.871828: {'loss': 0.7146316344738006, 'acc': 0.75025, 'val_loss': 0.8947272836023076, 'val_acc': 0.6978}\n",
      "Epoch 25 ends at 14:20:17.927736: {'loss': 0.7121123872756958, 'acc': 0.75215, 'val_loss': 0.8219224079779, 'val_acc': 0.7169}\n",
      "Epoch 26 ends at 14:22:00.250930: {'loss': 0.7033418053627014, 'acc': 0.753925, 'val_loss': 0.9609907053078816, 'val_acc': 0.6845}\n",
      "Epoch 27 ends at 14:23:42.077452: {'loss': 0.7032576812267304, 'acc': 0.75545, 'val_loss': 0.9439595436594289, 'val_acc': 0.6829}\n",
      "Epoch 28 ends at 14:25:24.352773: {'loss': 0.6933728209495544, 'acc': 0.756225, 'val_loss': 0.9000344766173393, 'val_acc': 0.6966}\n",
      "Epoch 29 ends at 14:27:11.483924: {'loss': 0.6888708562374115, 'acc': 0.7598, 'val_loss': 1.0246366049833358, 'val_acc': 0.662}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    epochs=30,\n",
    "                    validation_data=valid_data,\n",
    "                    verbose=0,\n",
    "                    callbacks=[cp_callback, MyCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 33ms/step - loss: 1.0632 - acc: 0.718 - 0s 26ms/step - loss: 1.0001 - acc: 0.734 - 0s 24ms/step - loss: 1.0158 - acc: 0.718 - 0s 23ms/step - loss: 0.9576 - acc: 0.734 - 0s 22ms/step - loss: 0.9502 - acc: 0.721 - 0s 22ms/step - loss: 0.9402 - acc: 0.713 - 0s 22ms/step - loss: 0.9489 - acc: 0.705 - 0s 22ms/step - loss: 0.9537 - acc: 0.707 - 0s 22ms/step - loss: 0.9659 - acc: 0.697 - 0s 22ms/step - loss: 0.9542 - acc: 0.704 - 0s 22ms/step - loss: 0.9283 - acc: 0.711 - 0s 22ms/step - loss: 0.9589 - acc: 0.704 - 0s 22ms/step - loss: 0.9753 - acc: 0.693 - 0s 22ms/step - loss: 0.9836 - acc: 0.690 - 0s 22ms/step - loss: 0.9935 - acc: 0.686 - 0s 22ms/step - loss: 1.0086 - acc: 0.686 - 0s 22ms/step - loss: 1.0153 - acc: 0.683 - 0s 22ms/step - loss: 1.0138 - acc: 0.681 - 0s 22ms/step - loss: 1.0144 - acc: 0.680 - 0s 22ms/step - loss: 1.0132 - acc: 0.678 - 0s 22ms/step - loss: 0.9983 - acc: 0.678 - 0s 22ms/step - loss: 0.9938 - acc: 0.683 - 1s 22ms/step - loss: 1.0101 - acc: 0.680 - 1s 22ms/step - loss: 1.0091 - acc: 0.679 - 1s 22ms/step - loss: 1.0129 - acc: 0.677 - 1s 22ms/step - loss: 1.0154 - acc: 0.679 - 1s 22ms/step - loss: 1.0118 - acc: 0.677 - 1s 22ms/step - loss: 1.0048 - acc: 0.681 - 1s 22ms/step - loss: 1.0061 - acc: 0.681 - 1s 22ms/step - loss: 1.0067 - acc: 0.679 - 1s 22ms/step - loss: 1.0063 - acc: 0.680 - 1s 22ms/step - loss: 0.9954 - acc: 0.683 - 1s 22ms/step - loss: 1.0145 - acc: 0.679 - 1s 22ms/step - loss: 1.0159 - acc: 0.677 - 1s 22ms/step - loss: 1.0086 - acc: 0.678 - 1s 22ms/step - loss: 1.0018 - acc: 0.680 - 1s 22ms/step - loss: 1.0081 - acc: 0.679 - 1s 22ms/step - loss: 1.0060 - acc: 0.681 - 1s 22ms/step - loss: 1.0029 - acc: 0.683 - 1s 22ms/step - loss: 1.0025 - acc: 0.684 - 1s 22ms/step - loss: 1.0027 - acc: 0.684 - 1s 22ms/step - loss: 0.9976 - acc: 0.686 - 1s 22ms/step - loss: 0.9962 - acc: 0.687 - 1s 22ms/step - loss: 0.9968 - acc: 0.684 - 1s 22ms/step - loss: 0.9956 - acc: 0.684 - 1s 22ms/step - loss: 0.9969 - acc: 0.682 - 1s 22ms/step - loss: 0.9966 - acc: 0.680 - 1s 22ms/step - loss: 0.9942 - acc: 0.680 - 1s 22ms/step - loss: 0.9935 - acc: 0.679 - 1s 22ms/step - loss: 0.9931 - acc: 0.680 - 1s 22ms/step - loss: 0.9908 - acc: 0.680 - 1s 22ms/step - loss: 0.9960 - acc: 0.678 - 1s 22ms/step - loss: 1.0013 - acc: 0.677 - 1s 22ms/step - loss: 0.9988 - acc: 0.678 - 1s 22ms/step - loss: 0.9957 - acc: 0.679 - 1s 22ms/step - loss: 0.9979 - acc: 0.679 - 1s 22ms/step - loss: 0.9972 - acc: 0.679 - 1s 22ms/step - loss: 0.9983 - acc: 0.679 - 1s 22ms/step - loss: 1.0011 - acc: 0.677 - 1s 22ms/step - loss: 1.0070 - acc: 0.677 - 1s 22ms/step - loss: 1.0123 - acc: 0.676 - 1s 22ms/step - loss: 1.0119 - acc: 0.675 - 1s 22ms/step - loss: 1.0136 - acc: 0.675 - 1s 22ms/step - loss: 1.0152 - acc: 0.674 - 1s 22ms/step - loss: 1.0173 - acc: 0.674 - 1s 22ms/step - loss: 1.0169 - acc: 0.673 - 1s 22ms/step - loss: 1.0218 - acc: 0.672 - 2s 22ms/step - loss: 1.0276 - acc: 0.670 - 2s 22ms/step - loss: 1.0240 - acc: 0.671 - 2s 22ms/step - loss: 1.0245 - acc: 0.670 - 2s 22ms/step - loss: 1.0278 - acc: 0.669 - 2s 22ms/step - loss: 1.0293 - acc: 0.668 - 2s 22ms/step - loss: 1.0320 - acc: 0.668 - 2s 22ms/step - loss: 1.0280 - acc: 0.668 - 2s 22ms/step - loss: 1.0291 - acc: 0.667 - 2s 22ms/step - loss: 1.0268 - acc: 0.668 - 2s 22ms/step - loss: 1.0263 - acc: 0.667 - 2s 22ms/step - loss: 1.0255 - acc: 0.667 - 2s 22ms/step - loss: 1.0224 - acc: 0.667 - 2s 22ms/step - loss: 1.0204 - acc: 0.668 - 2s 22ms/step - loss: 1.0199 - acc: 0.668 - 2s 22ms/step - loss: 1.0209 - acc: 0.668 - 2s 22ms/step - loss: 1.0194 - acc: 0.668 - 2s 22ms/step - loss: 1.0185 - acc: 0.668 - 2s 22ms/step - loss: 1.0179 - acc: 0.668 - 2s 22ms/step - loss: 1.0191 - acc: 0.667 - 2s 22ms/step - loss: 1.0195 - acc: 0.667 - 2s 22ms/step - loss: 1.0238 - acc: 0.666 - 2s 22ms/step - loss: 1.0229 - acc: 0.666 - 2s 22ms/step - loss: 1.0233 - acc: 0.666 - 2s 22ms/step - loss: 1.0220 - acc: 0.666 - 2s 22ms/step - loss: 1.0216 - acc: 0.667 - 2s 22ms/step - loss: 1.0195 - acc: 0.668 - 2s 22ms/step - loss: 1.0237 - acc: 0.667 - 2s 22ms/step - loss: 1.0227 - acc: 0.667 - 2s 22ms/step - loss: 1.0229 - acc: 0.667 - 2s 22ms/step - loss: 1.0248 - acc: 0.666 - 2s 22ms/step - loss: 1.0264 - acc: 0.666 - 2s 22ms/step - loss: 1.0287 - acc: 0.665 - 2s 22ms/step - loss: 1.0269 - acc: 0.665 - 2s 22ms/step - loss: 1.0284 - acc: 0.665 - 2s 22ms/step - loss: 1.0244 - acc: 0.667 - 2s 22ms/step - loss: 1.0245 - acc: 0.666 - 2s 22ms/step - loss: 1.0255 - acc: 0.665 - 2s 22ms/step - loss: 1.0247 - acc: 0.665 - 2s 22ms/step - loss: 1.0282 - acc: 0.664 - 2s 22ms/step - loss: 1.0275 - acc: 0.664 - 2s 22ms/step - loss: 1.0261 - acc: 0.665 - 2s 22ms/step - loss: 1.0272 - acc: 0.665 - 2s 22ms/step - loss: 1.0314 - acc: 0.664 - 2s 22ms/step - loss: 1.0280 - acc: 0.665 - 3s 22ms/step - loss: 1.0317 - acc: 0.664 - 3s 22ms/step - loss: 1.0337 - acc: 0.664 - 3s 22ms/step - loss: 1.0317 - acc: 0.665 - 3s 22ms/step - loss: 1.0295 - acc: 0.665 - 3s 22ms/step - loss: 1.0317 - acc: 0.665 - 3s 22ms/step - loss: 1.0308 - acc: 0.665 - 3s 22ms/step - loss: 1.0308 - acc: 0.665 - 3s 22ms/step - loss: 1.0296 - acc: 0.666 - 3s 22ms/step - loss: 1.0279 - acc: 0.666 - 3s 22ms/step - loss: 1.0287 - acc: 0.666 - 3s 22ms/step - loss: 1.0274 - acc: 0.666 - 3s 22ms/step - loss: 1.0261 - acc: 0.666 - 3s 22ms/step - loss: 1.0265 - acc: 0.667 - 3s 22ms/step - loss: 1.0269 - acc: 0.667 - 3s 22ms/step - loss: 1.0264 - acc: 0.667 - 3s 22ms/step - loss: 1.0270 - acc: 0.667 - 3s 22ms/step - loss: 1.0267 - acc: 0.667 - 3s 22ms/step - loss: 1.0279 - acc: 0.666 - 3s 22ms/step - loss: 1.0278 - acc: 0.666 - 3s 22ms/step - loss: 1.0274 - acc: 0.666 - 3s 22ms/step - loss: 1.0279 - acc: 0.666 - 3s 22ms/step - loss: 1.0288 - acc: 0.666 - 3s 22ms/step - loss: 1.0273 - acc: 0.666 - 3s 22ms/step - loss: 1.0281 - acc: 0.666 - 3s 22ms/step - loss: 1.0326 - acc: 0.665 - 3s 22ms/step - loss: 1.0361 - acc: 0.664 - 3s 22ms/step - loss: 1.0360 - acc: 0.664 - 3s 22ms/step - loss: 1.0373 - acc: 0.664 - 3s 22ms/step - loss: 1.0367 - acc: 0.664 - 3s 22ms/step - loss: 1.0364 - acc: 0.663 - 3s 22ms/step - loss: 1.0352 - acc: 0.664 - 3s 22ms/step - loss: 1.0355 - acc: 0.663 - 3s 22ms/step - loss: 1.0334 - acc: 0.664 - 3s 22ms/step - loss: 1.0337 - acc: 0.664 - 3s 22ms/step - loss: 1.0339 - acc: 0.664 - 3s 22ms/step - loss: 1.0339 - acc: 0.664 - 3s 22ms/step - loss: 1.0343 - acc: 0.664 - 3s 22ms/step - loss: 1.0334 - acc: 0.664 - 3s 22ms/step - loss: 1.0339 - acc: 0.664 - 3s 22ms/step - loss: 1.0329 - acc: 0.664 - 3s 22ms/step - loss: 1.0329 - acc: 0.664 - 3s 22ms/step - loss: 1.0319 - acc: 0.664 - 3s 22ms/step - loss: 1.0307 - acc: 0.664 - 3s 22ms/step - loss: 1.0311 - acc: 0.664 - 3s 22ms/step - loss: 1.0324 - acc: 0.664 - 4s 22ms/step - loss: 1.0342 - acc: 0.664 - 4s 22ms/step - loss: 1.0342 - acc: 0.6644"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0342283791797175, 0.6644]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.models.load_model('checkpoints/{}.hdf5'.format(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 30,890\n",
      "Trainable params: 30,762\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 131ms/step - loss: 0.8724 - acc: 0.73 - 0s 74ms/step - loss: 0.8360 - acc: 0.7656 - 0s 56ms/step - loss: 0.8636 - acc: 0.750 - 0s 47ms/step - loss: 0.8193 - acc: 0.757 - 0s 42ms/step - loss: 0.7991 - acc: 0.756 - 0s 38ms/step - loss: 0.7849 - acc: 0.760 - 0s 36ms/step - loss: 0.7737 - acc: 0.758 - 0s 34ms/step - loss: 0.7874 - acc: 0.753 - 0s 33ms/step - loss: 0.7934 - acc: 0.744 - 0s 32ms/step - loss: 0.7921 - acc: 0.743 - 0s 31ms/step - loss: 0.7750 - acc: 0.750 - 0s 30ms/step - loss: 0.7970 - acc: 0.740 - 0s 29ms/step - loss: 0.8038 - acc: 0.735 - 0s 29ms/step - loss: 0.8112 - acc: 0.732 - 0s 28ms/step - loss: 0.8115 - acc: 0.732 - 0s 28ms/step - loss: 0.8227 - acc: 0.731 - 0s 28ms/step - loss: 0.8359 - acc: 0.725 - 0s 27ms/step - loss: 0.8390 - acc: 0.723 - 1s 27ms/step - loss: 0.8375 - acc: 0.720 - 1s 27ms/step - loss: 0.8365 - acc: 0.720 - 1s 27ms/step - loss: 0.8197 - acc: 0.726 - 1s 26ms/step - loss: 0.8128 - acc: 0.729 - 1s 26ms/step - loss: 0.8203 - acc: 0.728 - 1s 26ms/step - loss: 0.8212 - acc: 0.727 - 1s 26ms/step - loss: 0.8209 - acc: 0.725 - 1s 26ms/step - loss: 0.8224 - acc: 0.726 - 1s 26ms/step - loss: 0.8203 - acc: 0.725 - 1s 25ms/step - loss: 0.8142 - acc: 0.726 - 1s 25ms/step - loss: 0.8125 - acc: 0.725 - 1s 25ms/step - loss: 0.8151 - acc: 0.725 - 1s 25ms/step - loss: 0.8139 - acc: 0.724 - 1s 25ms/step - loss: 0.8058 - acc: 0.727 - 1s 25ms/step - loss: 0.8177 - acc: 0.724 - 1s 25ms/step - loss: 0.8152 - acc: 0.724 - 1s 25ms/step - loss: 0.8126 - acc: 0.725 - 1s 25ms/step - loss: 0.8100 - acc: 0.726 - 1s 25ms/step - loss: 0.8168 - acc: 0.724 - 1s 25ms/step - loss: 0.8150 - acc: 0.724 - 1s 24ms/step - loss: 0.8148 - acc: 0.724 - 1s 24ms/step - loss: 0.8184 - acc: 0.723 - 1s 24ms/step - loss: 0.8163 - acc: 0.724 - 1s 24ms/step - loss: 0.8120 - acc: 0.725 - 1s 24ms/step - loss: 0.8124 - acc: 0.724 - 1s 24ms/step - loss: 0.8177 - acc: 0.721 - 1s 24ms/step - loss: 0.8160 - acc: 0.721 - 1s 24ms/step - loss: 0.8159 - acc: 0.721 - 1s 24ms/step - loss: 0.8148 - acc: 0.720 - 1s 24ms/step - loss: 0.8113 - acc: 0.721 - 1s 24ms/step - loss: 0.8106 - acc: 0.721 - 1s 24ms/step - loss: 0.8104 - acc: 0.721 - 1s 24ms/step - loss: 0.8099 - acc: 0.723 - 1s 24ms/step - loss: 0.8125 - acc: 0.723 - 1s 24ms/step - loss: 0.8159 - acc: 0.722 - 1s 24ms/step - loss: 0.8149 - acc: 0.724 - 1s 24ms/step - loss: 0.8115 - acc: 0.725 - 1s 24ms/step - loss: 0.8115 - acc: 0.725 - 1s 24ms/step - loss: 0.8082 - acc: 0.725 - 1s 24ms/step - loss: 0.8085 - acc: 0.726 - 1s 24ms/step - loss: 0.8082 - acc: 0.726 - 1s 24ms/step - loss: 0.8130 - acc: 0.725 - 1s 24ms/step - loss: 0.8127 - acc: 0.725 - 1s 24ms/step - loss: 0.8144 - acc: 0.725 - 1s 24ms/step - loss: 0.8152 - acc: 0.724 - 2s 24ms/step - loss: 0.8196 - acc: 0.723 - 2s 24ms/step - loss: 0.8213 - acc: 0.723 - 2s 23ms/step - loss: 0.8210 - acc: 0.723 - 2s 23ms/step - loss: 0.8249 - acc: 0.722 - 2s 23ms/step - loss: 0.8282 - acc: 0.720 - 2s 23ms/step - loss: 0.8254 - acc: 0.721 - 2s 23ms/step - loss: 0.8251 - acc: 0.721 - 2s 23ms/step - loss: 0.8264 - acc: 0.720 - 2s 23ms/step - loss: 0.8284 - acc: 0.720 - 2s 23ms/step - loss: 0.8286 - acc: 0.721 - 2s 23ms/step - loss: 0.8256 - acc: 0.722 - 2s 23ms/step - loss: 0.8262 - acc: 0.721 - 2s 23ms/step - loss: 0.8249 - acc: 0.721 - 2s 23ms/step - loss: 0.8248 - acc: 0.721 - 2s 23ms/step - loss: 0.8228 - acc: 0.721 - 2s 23ms/step - loss: 0.8222 - acc: 0.721 - 2s 23ms/step - loss: 0.8193 - acc: 0.722 - 2s 23ms/step - loss: 0.8207 - acc: 0.721 - 2s 23ms/step - loss: 0.8209 - acc: 0.722 - 2s 23ms/step - loss: 0.8201 - acc: 0.722 - 2s 23ms/step - loss: 0.8186 - acc: 0.722 - 2s 23ms/step - loss: 0.8178 - acc: 0.722 - 2s 23ms/step - loss: 0.8199 - acc: 0.721 - 2s 23ms/step - loss: 0.8184 - acc: 0.721 - 2s 23ms/step - loss: 0.8192 - acc: 0.720 - 2s 23ms/step - loss: 0.8180 - acc: 0.721 - 2s 23ms/step - loss: 0.8190 - acc: 0.720 - 2s 23ms/step - loss: 0.8175 - acc: 0.721 - 2s 23ms/step - loss: 0.8178 - acc: 0.721 - 2s 23ms/step - loss: 0.8160 - acc: 0.722 - 2s 23ms/step - loss: 0.8182 - acc: 0.721 - 2s 23ms/step - loss: 0.8185 - acc: 0.720 - 2s 23ms/step - loss: 0.8177 - acc: 0.721 - 2s 23ms/step - loss: 0.8199 - acc: 0.720 - 2s 23ms/step - loss: 0.8201 - acc: 0.721 - 2s 23ms/step - loss: 0.8214 - acc: 0.720 - 2s 23ms/step - loss: 0.8208 - acc: 0.721 - 2s 23ms/step - loss: 0.8219 - acc: 0.721 - 2s 23ms/step - loss: 0.8188 - acc: 0.722 - 2s 23ms/step - loss: 0.8188 - acc: 0.721 - 2s 23ms/step - loss: 0.8193 - acc: 0.721 - 2s 23ms/step - loss: 0.8194 - acc: 0.721 - 2s 23ms/step - loss: 0.8211 - acc: 0.720 - 2s 23ms/step - loss: 0.8205 - acc: 0.719 - 2s 23ms/step - loss: 0.8215 - acc: 0.720 - 2s 23ms/step - loss: 0.8215 - acc: 0.720 - 2s 23ms/step - loss: 0.8241 - acc: 0.719 - 3s 23ms/step - loss: 0.8219 - acc: 0.720 - 3s 23ms/step - loss: 0.8238 - acc: 0.719 - 3s 23ms/step - loss: 0.8268 - acc: 0.718 - 3s 23ms/step - loss: 0.8256 - acc: 0.719 - 3s 23ms/step - loss: 0.8254 - acc: 0.719 - 3s 23ms/step - loss: 0.8269 - acc: 0.719 - 3s 23ms/step - loss: 0.8263 - acc: 0.719 - 3s 23ms/step - loss: 0.8260 - acc: 0.720 - 3s 23ms/step - loss: 0.8253 - acc: 0.720 - 3s 23ms/step - loss: 0.8236 - acc: 0.721 - 3s 23ms/step - loss: 0.8233 - acc: 0.721 - 3s 23ms/step - loss: 0.8225 - acc: 0.721 - 3s 23ms/step - loss: 0.8214 - acc: 0.722 - 3s 23ms/step - loss: 0.8209 - acc: 0.722 - 3s 23ms/step - loss: 0.8204 - acc: 0.722 - 3s 23ms/step - loss: 0.8200 - acc: 0.723 - 3s 23ms/step - loss: 0.8207 - acc: 0.722 - 3s 23ms/step - loss: 0.8211 - acc: 0.723 - 3s 23ms/step - loss: 0.8217 - acc: 0.722 - 3s 23ms/step - loss: 0.8206 - acc: 0.723 - 3s 23ms/step - loss: 0.8205 - acc: 0.723 - 3s 23ms/step - loss: 0.8202 - acc: 0.723 - 3s 23ms/step - loss: 0.8212 - acc: 0.723 - 3s 23ms/step - loss: 0.8204 - acc: 0.723 - 3s 23ms/step - loss: 0.8200 - acc: 0.723 - 3s 23ms/step - loss: 0.8226 - acc: 0.723 - 3s 23ms/step - loss: 0.8257 - acc: 0.722 - 3s 23ms/step - loss: 0.8254 - acc: 0.722 - 3s 23ms/step - loss: 0.8270 - acc: 0.721 - 3s 23ms/step - loss: 0.8260 - acc: 0.722 - 3s 23ms/step - loss: 0.8259 - acc: 0.721 - 3s 23ms/step - loss: 0.8247 - acc: 0.721 - 3s 23ms/step - loss: 0.8251 - acc: 0.720 - 3s 23ms/step - loss: 0.8238 - acc: 0.721 - 3s 23ms/step - loss: 0.8236 - acc: 0.721 - 3s 23ms/step - loss: 0.8240 - acc: 0.720 - 3s 23ms/step - loss: 0.8237 - acc: 0.721 - 3s 23ms/step - loss: 0.8243 - acc: 0.720 - 3s 23ms/step - loss: 0.8236 - acc: 0.720 - 3s 23ms/step - loss: 0.8241 - acc: 0.720 - 3s 23ms/step - loss: 0.8231 - acc: 0.720 - 3s 23ms/step - loss: 0.8229 - acc: 0.720 - 3s 23ms/step - loss: 0.8220 - acc: 0.721 - 3s 23ms/step - loss: 0.8217 - acc: 0.721 - 4s 23ms/step - loss: 0.8219 - acc: 0.721 - 4s 23ms/step - loss: 0.8225 - acc: 0.721 - 4s 23ms/step - loss: 0.8221 - acc: 0.721 - 4s 23ms/step - loss: 0.8221 - acc: 0.7214"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8221499644646979, 0.7214]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 207ms/step - loss: 1.1617 - acc: 0.68 - 0s 113ms/step - loss: 0.8635 - acc: 0.74 - 0s 82ms/step - loss: 0.8534 - acc: 0.7292 - 0s 67ms/step - loss: 0.8202 - acc: 0.738 - 0s 58ms/step - loss: 0.8347 - acc: 0.734 - 0s 52ms/step - loss: 0.8541 - acc: 0.726 - 0s 47ms/step - loss: 0.8324 - acc: 0.734 - 0s 44ms/step - loss: 0.8266 - acc: 0.728 - 0s 42ms/step - loss: 0.8192 - acc: 0.730 - 0s 40ms/step - loss: 0.8173 - acc: 0.731 - 0s 38ms/step - loss: 0.8256 - acc: 0.725 - 0s 36ms/step - loss: 0.8186 - acc: 0.729 - 0s 35ms/step - loss: 0.8023 - acc: 0.730 - 0s 34ms/step - loss: 0.8019 - acc: 0.725 - 1s 33ms/step - loss: 0.8059 - acc: 0.724 - 1s 33ms/step - loss: 0.8055 - acc: 0.726 - 1s 32ms/step - loss: 0.7955 - acc: 0.731 - 1s 31ms/step - loss: 0.8013 - acc: 0.730 - 1s 31ms/step - loss: 0.7948 - acc: 0.731 - 1s 30ms/step - loss: 0.7805 - acc: 0.735 - 1s 30ms/step - loss: 0.7859 - acc: 0.730 - 1s 30ms/step - loss: 0.7877 - acc: 0.732 - 1s 29ms/step - loss: 0.7784 - acc: 0.735 - 1s 29ms/step - loss: 0.7835 - acc: 0.733 - 1s 29ms/step - loss: 0.7819 - acc: 0.732 - 1s 28ms/step - loss: 0.7835 - acc: 0.731 - 1s 28ms/step - loss: 0.7831 - acc: 0.731 - 1s 28ms/step - loss: 0.7848 - acc: 0.729 - 1s 28ms/step - loss: 0.7760 - acc: 0.734 - 1s 27ms/step - loss: 0.7764 - acc: 0.735 - 1s 27ms/step - loss: 0.7794 - acc: 0.734 - 1s 27ms/step - loss: 0.7754 - acc: 0.734 - 1s 27ms/step - loss: 0.7793 - acc: 0.733 - 1s 27ms/step - loss: 0.7789 - acc: 0.733 - 1s 27ms/step - loss: 0.7814 - acc: 0.734 - 1s 26ms/step - loss: 0.7879 - acc: 0.731 - 1s 26ms/step - loss: 0.7978 - acc: 0.729 - 1s 26ms/step - loss: 0.7984 - acc: 0.730 - 1s 26ms/step - loss: 0.7941 - acc: 0.730 - 1s 26ms/step - loss: 0.7944 - acc: 0.730 - 1s 26ms/step - loss: 0.7940 - acc: 0.730 - 1s 26ms/step - loss: 0.7988 - acc: 0.731 - 1s 26ms/step - loss: 0.7952 - acc: 0.732 - 1s 26ms/step - loss: 0.7984 - acc: 0.731 - 1s 26ms/step - loss: 0.7958 - acc: 0.731 - 1s 25ms/step - loss: 0.8009 - acc: 0.729 - 1s 25ms/step - loss: 0.7997 - acc: 0.729 - 1s 25ms/step - loss: 0.7991 - acc: 0.729 - 1s 25ms/step - loss: 0.7996 - acc: 0.729 - 1s 25ms/step - loss: 0.7997 - acc: 0.730 - 1s 25ms/step - loss: 0.8005 - acc: 0.728 - 1s 25ms/step - loss: 0.8036 - acc: 0.727 - 1s 25ms/step - loss: 0.8049 - acc: 0.727 - 1s 25ms/step - loss: 0.8065 - acc: 0.725 - 1s 25ms/step - loss: 0.8049 - acc: 0.725 - 1s 25ms/step - loss: 0.8058 - acc: 0.725 - 1s 25ms/step - loss: 0.8054 - acc: 0.724 - 1s 25ms/step - loss: 0.8037 - acc: 0.725 - 1s 25ms/step - loss: 0.8051 - acc: 0.724 - 1s 25ms/step - loss: 0.8056 - acc: 0.725 - 2s 25ms/step - loss: 0.8087 - acc: 0.724 - 2s 25ms/step - loss: 0.8106 - acc: 0.723 - 2s 25ms/step - loss: 0.8088 - acc: 0.724 - 2s 25ms/step - loss: 0.8113 - acc: 0.725 - 2s 25ms/step - loss: 0.8101 - acc: 0.725 - 2s 25ms/step - loss: 0.8096 - acc: 0.725 - 2s 25ms/step - loss: 0.8125 - acc: 0.723 - 2s 25ms/step - loss: 0.8107 - acc: 0.724 - 2s 25ms/step - loss: 0.8130 - acc: 0.723 - 2s 24ms/step - loss: 0.8149 - acc: 0.722 - 2s 24ms/step - loss: 0.8154 - acc: 0.722 - 2s 24ms/step - loss: 0.8159 - acc: 0.721 - 2s 24ms/step - loss: 0.8153 - acc: 0.721 - 2s 24ms/step - loss: 0.8191 - acc: 0.720 - 2s 24ms/step - loss: 0.8190 - acc: 0.720 - 2s 24ms/step - loss: 0.8171 - acc: 0.720 - 2s 24ms/step - loss: 0.8180 - acc: 0.719 - 2s 24ms/step - loss: 0.8164 - acc: 0.720 - 2s 24ms/step - loss: 0.8183 - acc: 0.719 - 2s 24ms/step - loss: 0.8192 - acc: 0.718 - 2s 24ms/step - loss: 0.8217 - acc: 0.717 - 2s 24ms/step - loss: 0.8218 - acc: 0.716 - 2s 24ms/step - loss: 0.8214 - acc: 0.717 - 2s 24ms/step - loss: 0.8212 - acc: 0.717 - 2s 24ms/step - loss: 0.8236 - acc: 0.717 - 2s 24ms/step - loss: 0.8224 - acc: 0.717 - 2s 24ms/step - loss: 0.8215 - acc: 0.717 - 2s 24ms/step - loss: 0.8252 - acc: 0.715 - 2s 24ms/step - loss: 0.8240 - acc: 0.715 - 2s 24ms/step - loss: 0.8235 - acc: 0.715 - 2s 24ms/step - loss: 0.8229 - acc: 0.715 - 2s 24ms/step - loss: 0.8226 - acc: 0.716 - 2s 24ms/step - loss: 0.8234 - acc: 0.715 - 2s 24ms/step - loss: 0.8227 - acc: 0.715 - 2s 24ms/step - loss: 0.8241 - acc: 0.714 - 2s 24ms/step - loss: 0.8255 - acc: 0.714 - 2s 24ms/step - loss: 0.8265 - acc: 0.713 - 2s 24ms/step - loss: 0.8240 - acc: 0.714 - 2s 24ms/step - loss: 0.8232 - acc: 0.714 - 2s 24ms/step - loss: 0.8260 - acc: 0.714 - 2s 24ms/step - loss: 0.8243 - acc: 0.715 - 2s 24ms/step - loss: 0.8247 - acc: 0.716 - 2s 24ms/step - loss: 0.8250 - acc: 0.716 - 2s 24ms/step - loss: 0.8251 - acc: 0.715 - 2s 24ms/step - loss: 0.8233 - acc: 0.716 - 2s 24ms/step - loss: 0.8227 - acc: 0.716 - 3s 23ms/step - loss: 0.8232 - acc: 0.716 - 3s 23ms/step - loss: 0.8248 - acc: 0.716 - 3s 23ms/step - loss: 0.8266 - acc: 0.715 - 3s 23ms/step - loss: 0.8266 - acc: 0.715 - 3s 23ms/step - loss: 0.8281 - acc: 0.715 - 3s 23ms/step - loss: 0.8282 - acc: 0.715 - 3s 23ms/step - loss: 0.8321 - acc: 0.714 - 3s 23ms/step - loss: 0.8321 - acc: 0.713 - 3s 23ms/step - loss: 0.8332 - acc: 0.713 - 3s 23ms/step - loss: 0.8322 - acc: 0.714 - 3s 23ms/step - loss: 0.8312 - acc: 0.714 - 3s 23ms/step - loss: 0.8322 - acc: 0.713 - 3s 23ms/step - loss: 0.8336 - acc: 0.713 - 3s 23ms/step - loss: 0.8318 - acc: 0.713 - 3s 23ms/step - loss: 0.8319 - acc: 0.713 - 3s 23ms/step - loss: 0.8320 - acc: 0.713 - 3s 23ms/step - loss: 0.8312 - acc: 0.714 - 3s 23ms/step - loss: 0.8320 - acc: 0.713 - 3s 23ms/step - loss: 0.8303 - acc: 0.714 - 3s 23ms/step - loss: 0.8300 - acc: 0.714 - 3s 23ms/step - loss: 0.8284 - acc: 0.714 - 3s 23ms/step - loss: 0.8297 - acc: 0.714 - 3s 23ms/step - loss: 0.8288 - acc: 0.714 - 3s 23ms/step - loss: 0.8294 - acc: 0.713 - 3s 23ms/step - loss: 0.8291 - acc: 0.714 - 3s 23ms/step - loss: 0.8292 - acc: 0.713 - 3s 23ms/step - loss: 0.8286 - acc: 0.714 - 3s 23ms/step - loss: 0.8288 - acc: 0.714 - 3s 23ms/step - loss: 0.8282 - acc: 0.714 - 3s 23ms/step - loss: 0.8256 - acc: 0.715 - 3s 23ms/step - loss: 0.8252 - acc: 0.715 - 3s 23ms/step - loss: 0.8256 - acc: 0.714 - 3s 23ms/step - loss: 0.8259 - acc: 0.714 - 3s 23ms/step - loss: 0.8254 - acc: 0.715 - 3s 23ms/step - loss: 0.8245 - acc: 0.715 - 3s 23ms/step - loss: 0.8234 - acc: 0.715 - 3s 23ms/step - loss: 0.8243 - acc: 0.715 - 3s 23ms/step - loss: 0.8242 - acc: 0.715 - 3s 23ms/step - loss: 0.8231 - acc: 0.716 - 3s 23ms/step - loss: 0.8221 - acc: 0.716 - 3s 23ms/step - loss: 0.8223 - acc: 0.716 - 3s 23ms/step - loss: 0.8226 - acc: 0.716 - 3s 23ms/step - loss: 0.8234 - acc: 0.716 - 3s 23ms/step - loss: 0.8237 - acc: 0.716 - 3s 23ms/step - loss: 0.8249 - acc: 0.716 - 3s 23ms/step - loss: 0.8247 - acc: 0.716 - 4s 23ms/step - loss: 0.8242 - acc: 0.716 - 4s 23ms/step - loss: 0.8233 - acc: 0.716 - 4s 23ms/step - loss: 0.8234 - acc: 0.716 - 4s 23ms/step - loss: 0.8230 - acc: 0.716 - 4s 23ms/step - loss: 0.8219 - acc: 0.716 - 4s 23ms/step - loss: 0.8219 - acc: 0.7169"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8219224079779, 0.7169]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
