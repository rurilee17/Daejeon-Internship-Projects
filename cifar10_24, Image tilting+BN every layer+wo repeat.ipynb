{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\affinity\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: promise in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.2.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (3.9.2)\n",
      "Requirement already satisfied: termcolor in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.11.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.22.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (4.36.1)\n",
      "Requirement already satisfied: future in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.17.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.8.0)\n",
      "Requirement already satisfied: dill in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.3.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.16.5)\n",
      "Requirement already satisfied: attrs in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (19.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-datasets) (41.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
      "Requirement already satisfied: googleapis-common-protos in c:\\users\\affinity\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\affinity\\anaconda3\\lib\\site-packages (19.3.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "from keras import regularizers\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfds.load(name='cifar10:3.*.*', split='train[:80%]', as_supervised=True)\n",
    "valid_data = tfds.load(name='cifar10:3.*.*', split='train[80%:]', as_supervised=True)\n",
    "test_data = tfds.load(name='cifar10:3.*.*', split='test', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_contrast(image, lower=2.0, upper = 4.0)\n",
    "    image = tf.image.random_hue(image, max_delta = 0.8)\n",
    "    image = tf.image.random_saturation(image, lower=2.0, upper=4.0)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 5,638,154\n",
      "Trainable params: 5,633,802\n",
      "Non-trainable params: 4,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, 3, kernel_regularizer=regularizers.l2(0.001), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),  # Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나이다\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, kernel_regularizer=regularizers.l2(0.001),activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, 3,kernel_regularizer=regularizers.l2(0.001), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv2D(256, 3,kernel_regularizer=regularizers.l2(0.001), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, 3, kernel_regularizer=regularizers.l2(0.001),activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv2D(512, 3,kernel_regularizer=regularizers.l2(0.001), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(512, 3, kernel_regularizer=regularizers.l2(0.001),activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "train_data = train_data.map(scale).shuffle(50000).batch(64) \n",
    "valid_data = valid_data.map(scale).batch(64)\n",
    "test_data = test_data.map(scale).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(   \n",
    "    'checkpoints/{}.hdf5'.format(dt),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_start(self, epoch, logs=None):\n",
    "        print('Epoch {} begins at {}: {}'.format(epoch, datetime.now().time(), logs))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {} ends at {}: {}'.format(epoch, datetime.now().time(), logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ends at 10:04:40.042274: {'loss': 3.8071762252807617, 'accuracy': 0.421025, 'val_loss': 3.303182456144102, 'val_accuracy': 0.3731}\n",
      "Epoch 1 ends at 10:05:42.004615: {'loss': 3.1255492015838624, 'accuracy': 0.5197, 'val_loss': 3.817743650667227, 'val_accuracy': 0.2738}\n",
      "Epoch 2 ends at 10:06:52.270734: {'loss': 2.6038786796569826, 'accuracy': 0.5904, 'val_loss': 3.3474865308992423, 'val_accuracy': 0.3262}\n",
      "Epoch 3 ends at 10:08:00.622981: {'loss': 2.453968006324768, 'accuracy': 0.621175, 'val_loss': 2.934883524657814, 'val_accuracy': 0.4361}\n",
      "Epoch 4 ends at 10:09:07.691226: {'loss': 2.420680487251282, 'accuracy': 0.641775, 'val_loss': 2.997026519410929, 'val_accuracy': 0.4183}\n",
      "Epoch 5 ends at 10:10:16.562954: {'loss': 2.4406861267089845, 'accuracy': 0.652, 'val_loss': 3.097807302596463, 'val_accuracy': 0.4603}\n",
      "Epoch 6 ends at 10:11:25.727692: {'loss': 2.3174808221817016, 'accuracy': 0.672325, 'val_loss': 2.363996654559093, 'val_accuracy': 0.6045}\n",
      "Epoch 7 ends at 10:12:34.500068: {'loss': 2.2137103643417357, 'accuracy': 0.685775, 'val_loss': 2.7438790479283424, 'val_accuracy': 0.5054}\n",
      "Epoch 8 ends at 10:13:44.473296: {'loss': 2.2642599340438845, 'accuracy': 0.6869, 'val_loss': 2.24446933816193, 'val_accuracy': 0.6162}\n",
      "Epoch 9 ends at 10:14:54.413364: {'loss': 2.0036927001953124, 'accuracy': 0.709025, 'val_loss': 2.0434938001025253, 'val_accuracy': 0.6865}\n",
      "Epoch 10 ends at 10:16:04.874006: {'loss': 1.9721552143096923, 'accuracy': 0.710475, 'val_loss': 2.2615884656359437, 'val_accuracy': 0.623}\n",
      "Epoch 11 ends at 10:17:16.033687: {'loss': 1.8838145278930665, 'accuracy': 0.7211, 'val_loss': 3.222279269224519, 'val_accuracy': 0.3807}\n",
      "Epoch 12 ends at 10:18:27.710083: {'loss': 1.8613358791351318, 'accuracy': 0.73095, 'val_loss': 2.069636293277619, 'val_accuracy': 0.6173}\n",
      "Epoch 13 ends at 10:19:44.224657: {'loss': 1.7094117723464965, 'accuracy': 0.74375, 'val_loss': 1.640135364927304, 'val_accuracy': 0.7448}\n",
      "Epoch 14 ends at 10:21:03.724157: {'loss': 1.5818313003540039, 'accuracy': 0.75795, 'val_loss': 1.6870755253324083, 'val_accuracy': 0.7194}\n",
      "Epoch 15 ends at 10:22:17.036571: {'loss': 1.5540056154251098, 'accuracy': 0.763725, 'val_loss': 1.5997579963343918, 'val_accuracy': 0.7387}\n",
      "Epoch 16 ends at 10:23:30.757809: {'loss': 1.5070311782836914, 'accuracy': 0.7679, 'val_loss': 1.6301581297710444, 'val_accuracy': 0.733}\n",
      "Epoch 17 ends at 10:24:46.278540: {'loss': 1.4458537433624268, 'accuracy': 0.777375, 'val_loss': 1.6299590396273667, 'val_accuracy': 0.7203}\n",
      "Epoch 18 ends at 10:26:02.443499: {'loss': 1.3765947151184081, 'accuracy': 0.7869, 'val_loss': 1.4497222756124606, 'val_accuracy': 0.7527}\n",
      "Epoch 19 ends at 10:27:18.208046: {'loss': 1.3498472330093383, 'accuracy': 0.791125, 'val_loss': 1.5657307866272654, 'val_accuracy': 0.7079}\n",
      "Epoch 20 ends at 10:28:33.254274: {'loss': 1.3023904458999633, 'accuracy': 0.79715, 'val_loss': 1.4618980952888538, 'val_accuracy': 0.7419}\n",
      "Epoch 21 ends at 10:29:48.642411: {'loss': 1.2634831253051757, 'accuracy': 0.801, 'val_loss': 1.4532845665694802, 'val_accuracy': 0.7332}\n",
      "Epoch 22 ends at 10:31:04.558937: {'loss': 1.2262475173950196, 'accuracy': 0.80735, 'val_loss': 1.4770247366777651, 'val_accuracy': 0.723}\n",
      "Epoch 23 ends at 10:32:20.584576: {'loss': 1.1965830116271972, 'accuracy': 0.810475, 'val_loss': 1.578600661769794, 'val_accuracy': 0.6841}\n",
      "Epoch 24 ends at 10:33:38.044056: {'loss': 1.1952362840652466, 'accuracy': 0.810875, 'val_loss': 1.3059310123419305, 'val_accuracy': 0.7695}\n",
      "Epoch 25 ends at 10:34:53.933796: {'loss': 1.1705134016036987, 'accuracy': 0.81405, 'val_loss': 1.38686199248976, 'val_accuracy': 0.7444}\n",
      "Epoch 26 ends at 10:36:10.266162: {'loss': 1.1468169854164123, 'accuracy': 0.8179, 'val_loss': 1.2585681600935141, 'val_accuracy': 0.778}\n",
      "Epoch 27 ends at 10:37:24.954146: {'loss': 1.1277571484565734, 'accuracy': 0.82025, 'val_loss': 1.423584434636839, 'val_accuracy': 0.731}\n",
      "Epoch 28 ends at 10:38:40.209202: {'loss': 1.12362132730484, 'accuracy': 0.8227, 'val_loss': 1.3301212787628174, 'val_accuracy': 0.7444}\n",
      "Epoch 29 ends at 10:39:56.091151: {'loss': 1.1034061889648437, 'accuracy': 0.823225, 'val_loss': 1.3000071500517, 'val_accuracy': 0.7581}\n",
      "Epoch 30 ends at 10:41:12.805108: {'loss': 1.1063446831703185, 'accuracy': 0.82245, 'val_loss': 1.211704727570722, 'val_accuracy': 0.7915}\n",
      "Epoch 31 ends at 10:42:28.633505: {'loss': 1.079595660018921, 'accuracy': 0.829125, 'val_loss': 1.3208755892553148, 'val_accuracy': 0.7447}\n",
      "Epoch 32 ends at 10:43:44.392596: {'loss': 1.06489867105484, 'accuracy': 0.831075, 'val_loss': 1.1243159805133844, 'val_accuracy': 0.8068}\n",
      "Epoch 33 ends at 10:45:00.053584: {'loss': 1.0607436818122864, 'accuracy': 0.831425, 'val_loss': 1.1734821933090307, 'val_accuracy': 0.7977}\n",
      "Epoch 34 ends at 10:46:15.430046: {'loss': 1.0598583749771118, 'accuracy': 0.8318, 'val_loss': 1.1458568341413122, 'val_accuracy': 0.7972}\n",
      "Epoch 35 ends at 10:47:31.800119: {'loss': 1.0480453273773194, 'accuracy': 0.833525, 'val_loss': 1.159531873502549, 'val_accuracy': 0.7929}\n",
      "Epoch 36 ends at 10:48:52.043013: {'loss': 1.046750845146179, 'accuracy': 0.830675, 'val_loss': 1.2083372113051687, 'val_accuracy': 0.7832}\n",
      "Epoch 37 ends at 10:50:10.917697: {'loss': 1.03161556224823, 'accuracy': 0.837125, 'val_loss': 1.1463065568808537, 'val_accuracy': 0.796}\n",
      "Epoch 38 ends at 10:51:28.529964: {'loss': 1.0261666716575621, 'accuracy': 0.836975, 'val_loss': 1.2293142782654731, 'val_accuracy': 0.7679}\n",
      "Epoch 39 ends at 10:52:47.487549: {'loss': 1.025632076740265, 'accuracy': 0.835175, 'val_loss': 1.2627015011325764, 'val_accuracy': 0.7702}\n",
      "Epoch 40 ends at 10:54:08.103554: {'loss': 1.0067299946784973, 'accuracy': 0.841225, 'val_loss': 1.2038907833919403, 'val_accuracy': 0.7799}\n",
      "Epoch 41 ends at 10:55:25.442353: {'loss': 1.0102585742950438, 'accuracy': 0.83675, 'val_loss': 1.2118439647802122, 'val_accuracy': 0.7637}\n",
      "Epoch 42 ends at 10:56:39.428571: {'loss': 0.9983819741249085, 'accuracy': 0.838825, 'val_loss': 1.1234930632220712, 'val_accuracy': 0.7966}\n",
      "Epoch 43 ends at 10:57:53.207250: {'loss': 0.9935612147331238, 'accuracy': 0.839475, 'val_loss': 1.3654735365491004, 'val_accuracy': 0.718}\n",
      "Epoch 44 ends at 10:59:07.127743: {'loss': 0.9952997859001159, 'accuracy': 0.841125, 'val_loss': 1.1556919703058377, 'val_accuracy': 0.7932}\n",
      "Epoch 45 ends at 11:00:25.713978: {'loss': 0.9875522498130799, 'accuracy': 0.84145, 'val_loss': 1.142524555989891, 'val_accuracy': 0.7904}\n",
      "Epoch 46 ends at 11:01:42.493133: {'loss': 0.9825004914283753, 'accuracy': 0.843875, 'val_loss': 1.1515760326841076, 'val_accuracy': 0.7928}\n",
      "Epoch 47 ends at 11:02:59.207313: {'loss': 0.9757432396888733, 'accuracy': 0.84595, 'val_loss': 1.2563057241925768, 'val_accuracy': 0.7595}\n",
      "Epoch 48 ends at 11:04:14.542111: {'loss': 0.9711390351295471, 'accuracy': 0.84645, 'val_loss': 1.1509641329194331, 'val_accuracy': 0.7853}\n",
      "Epoch 49 ends at 11:05:36.164417: {'loss': 0.9671614819526673, 'accuracy': 0.846625, 'val_loss': 1.172572582770305, 'val_accuracy': 0.7812}\n",
      "Epoch 50 ends at 11:06:54.491968: {'loss': 0.9714524770736694, 'accuracy': 0.8439, 'val_loss': 1.0802443794384124, 'val_accuracy': 0.8072}\n",
      "Epoch 51 ends at 11:08:11.081811: {'loss': 0.9603398533821106, 'accuracy': 0.845425, 'val_loss': 1.2433261328442082, 'val_accuracy': 0.7619}\n",
      "Epoch 52 ends at 11:09:27.800108: {'loss': 0.9680783992767334, 'accuracy': 0.8461, 'val_loss': 1.0461647874990088, 'val_accuracy': 0.8213}\n",
      "Epoch 53 ends at 11:10:42.323447: {'loss': 0.9589740170478821, 'accuracy': 0.847175, 'val_loss': 1.192627002099517, 'val_accuracy': 0.7822}\n",
      "Epoch 54 ends at 11:11:58.648296: {'loss': 0.9548390644073487, 'accuracy': 0.848875, 'val_loss': 1.224855544460807, 'val_accuracy': 0.766}\n",
      "Epoch 55 ends at 11:13:17.220902: {'loss': 0.9530893100738526, 'accuracy': 0.85005, 'val_loss': 1.1233873405274313, 'val_accuracy': 0.7873}\n",
      "Epoch 56 ends at 11:14:38.048955: {'loss': 0.9458283089637757, 'accuracy': 0.85025, 'val_loss': 1.1251983156629428, 'val_accuracy': 0.7967}\n",
      "Epoch 57 ends at 11:15:55.968473: {'loss': 0.9471160707473755, 'accuracy': 0.8509, 'val_loss': 1.2135823858771355, 'val_accuracy': 0.7762}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 ends at 11:17:15.829559: {'loss': 0.9477567176818847, 'accuracy': 0.848625, 'val_loss': 1.120626088160618, 'val_accuracy': 0.7984}\n",
      "Epoch 59 ends at 11:18:34.866593: {'loss': 0.9564913302421569, 'accuracy': 0.847275, 'val_loss': 1.1431157573772843, 'val_accuracy': 0.7888}\n",
      "Epoch 60 ends at 11:19:55.358901: {'loss': 0.9446400838851928, 'accuracy': 0.850975, 'val_loss': 1.0840496382895548, 'val_accuracy': 0.8077}\n",
      "Epoch 61 ends at 11:21:13.899789: {'loss': 0.948911387348175, 'accuracy': 0.850575, 'val_loss': 1.3409168462085117, 'val_accuracy': 0.736}\n",
      "Epoch 62 ends at 11:22:31.837257: {'loss': 0.9385342192649841, 'accuracy': 0.85145, 'val_loss': 1.2868560799367867, 'val_accuracy': 0.7434}\n",
      "Epoch 63 ends at 11:23:51.399341: {'loss': 0.942238445854187, 'accuracy': 0.84945, 'val_loss': 1.10017920755277, 'val_accuracy': 0.7939}\n",
      "Epoch 64 ends at 11:25:12.186048: {'loss': 0.9335017906188965, 'accuracy': 0.852675, 'val_loss': 1.1759325041892423, 'val_accuracy': 0.7834}\n",
      "Epoch 65 ends at 11:26:33.904698: {'loss': 0.9296690837860108, 'accuracy': 0.85285, 'val_loss': 1.0439909575091806, 'val_accuracy': 0.8226}\n",
      "Epoch 66 ends at 11:27:54.992373: {'loss': 0.9295205048561096, 'accuracy': 0.85305, 'val_loss': 1.0354256747634547, 'val_accuracy': 0.8202}\n",
      "Epoch 67 ends at 11:29:15.410635: {'loss': 0.9277684864044189, 'accuracy': 0.853575, 'val_loss': 1.2381436509691226, 'val_accuracy': 0.7568}\n",
      "Epoch 68 ends at 11:30:36.111271: {'loss': 0.9225370484352112, 'accuracy': 0.854075, 'val_loss': 1.0882481716241046, 'val_accuracy': 0.8002}\n",
      "Epoch 69 ends at 11:31:55.785918: {'loss': 0.9191929672241211, 'accuracy': 0.85695, 'val_loss': 1.0613533797537444, 'val_accuracy': 0.8149}\n",
      "Epoch 70 ends at 11:33:12.671693: {'loss': 0.9272111275672913, 'accuracy': 0.8521, 'val_loss': 1.19114760038959, 'val_accuracy': 0.775}\n",
      "Epoch 71 ends at 11:34:29.070794: {'loss': 0.9228831388473511, 'accuracy': 0.8571, 'val_loss': 1.2950322278745614, 'val_accuracy': 0.7467}\n",
      "Epoch 72 ends at 11:35:47.379560: {'loss': 0.9187825673103333, 'accuracy': 0.855475, 'val_loss': 1.5058527782464484, 'val_accuracy': 0.6922}\n",
      "Epoch 73 ends at 11:37:09.478496: {'loss': 0.9254090308189392, 'accuracy': 0.8555, 'val_loss': 1.094499606615419, 'val_accuracy': 0.8028}\n",
      "Epoch 74 ends at 11:38:31.268867: {'loss': 0.9116038546562195, 'accuracy': 0.857225, 'val_loss': 1.065271115986405, 'val_accuracy': 0.8126}\n",
      "Epoch 75 ends at 11:39:51.176406: {'loss': 0.9188576139450073, 'accuracy': 0.856225, 'val_loss': 1.0828785919080115, 'val_accuracy': 0.8061}\n",
      "Epoch 76 ends at 11:41:09.571637: {'loss': 0.9150104655265808, 'accuracy': 0.85665, 'val_loss': 1.071164820983911, 'val_accuracy': 0.8087}\n",
      "Epoch 77 ends at 11:42:28.512032: {'loss': 0.9179839237213134, 'accuracy': 0.853925, 'val_loss': 1.0126251908624249, 'val_accuracy': 0.8258}\n",
      "Epoch 78 ends at 11:43:46.340065: {'loss': 0.9139823716163635, 'accuracy': 0.85615, 'val_loss': 1.2147713755346408, 'val_accuracy': 0.7529}\n",
      "Epoch 79 ends at 11:45:05.775281: {'loss': 0.9106335297584534, 'accuracy': 0.8548, 'val_loss': 1.0518809173517167, 'val_accuracy': 0.8094}\n",
      "Epoch 80 ends at 11:46:25.778037: {'loss': 0.9122885245323181, 'accuracy': 0.8577, 'val_loss': 1.0505180058965258, 'val_accuracy': 0.8184}\n",
      "Epoch 81 ends at 11:47:43.791598: {'loss': 0.9105976913452148, 'accuracy': 0.8562, 'val_loss': 1.0738816804187312, 'val_accuracy': 0.8025}\n",
      "Epoch 82 ends at 11:49:02.431855: {'loss': 0.9048435559272766, 'accuracy': 0.8563, 'val_loss': 1.2168374862640527, 'val_accuracy': 0.7544}\n",
      "Epoch 83 ends at 11:50:21.891371: {'loss': 0.9112816691398621, 'accuracy': 0.85555, 'val_loss': 1.0700780244389916, 'val_accuracy': 0.8131}\n",
      "Epoch 84 ends at 11:51:40.809151: {'loss': 0.8971605435371399, 'accuracy': 0.86005, 'val_loss': 1.1742090058934158, 'val_accuracy': 0.7738}\n",
      "Epoch 85 ends at 11:52:57.096294: {'loss': 0.9019106434822083, 'accuracy': 0.85935, 'val_loss': 1.0859451992496563, 'val_accuracy': 0.7977}\n",
      "Epoch 86 ends at 11:54:13.985158: {'loss': 0.9028602887153625, 'accuracy': 0.860375, 'val_loss': 1.0420622168832523, 'val_accuracy': 0.8157}\n",
      "Epoch 87 ends at 11:55:31.521924: {'loss': 0.9009704229354858, 'accuracy': 0.859575, 'val_loss': 1.197519427272165, 'val_accuracy': 0.77}\n",
      "Epoch 88 ends at 11:56:49.012181: {'loss': 0.9045538175582886, 'accuracy': 0.855675, 'val_loss': 1.0470747856577491, 'val_accuracy': 0.8151}\n",
      "Epoch 89 ends at 11:58:08.242199: {'loss': 0.8984961894035339, 'accuracy': 0.857525, 'val_loss': 1.0640977161705114, 'val_accuracy': 0.8082}\n",
      "Epoch 90 ends at 11:59:28.213703: {'loss': 0.8958424751281738, 'accuracy': 0.8584, 'val_loss': 1.1216850766710416, 'val_accuracy': 0.7859}\n",
      "Epoch 91 ends at 12:00:51.021021: {'loss': 0.8955526086807251, 'accuracy': 0.8588, 'val_loss': 1.0494975348946396, 'val_accuracy': 0.8137}\n",
      "Epoch 92 ends at 12:02:11.405063: {'loss': 0.8880856114387512, 'accuracy': 0.861225, 'val_loss': 1.0834069229235315, 'val_accuracy': 0.799}\n",
      "Epoch 93 ends at 12:03:29.847890: {'loss': 0.8908716002464294, 'accuracy': 0.85905, 'val_loss': 1.0712948574382029, 'val_accuracy': 0.8041}\n",
      "Epoch 94 ends at 12:04:47.895574: {'loss': 0.9015828211784362, 'accuracy': 0.855375, 'val_loss': 1.0467182576276695, 'val_accuracy': 0.8151}\n",
      "Epoch 95 ends at 12:06:05.186533: {'loss': 0.8929883689880371, 'accuracy': 0.860225, 'val_loss': 1.0543865861406752, 'val_accuracy': 0.81}\n",
      "Epoch 96 ends at 12:07:23.885801: {'loss': 0.8896668064117431, 'accuracy': 0.860575, 'val_loss': 1.2544960091068487, 'val_accuracy': 0.7546}\n",
      "Epoch 97 ends at 12:08:41.971255: {'loss': 0.8857879955291748, 'accuracy': 0.859375, 'val_loss': 1.0265024832099865, 'val_accuracy': 0.8161}\n",
      "Epoch 98 ends at 12:10:00.084768: {'loss': 0.8853054564476013, 'accuracy': 0.86025, 'val_loss': 1.1127037766632761, 'val_accuracy': 0.7933}\n",
      "Epoch 99 ends at 12:11:19.028806: {'loss': 0.8869475825309754, 'accuracy': 0.86085, 'val_loss': 1.0215946845947557, 'val_accuracy': 0.8132}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    epochs=100,\n",
    "                    validation_data=valid_data,\n",
    "                    verbose=0,\n",
    "                    callbacks=[cp_callback, MyCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================]1.0683 - accuracy: 0.765 - 0s 110ms/step - loss: 1.0348 - accuracy: 0.781 - 0s 93ms/step - loss: 0.9620 - accuracy: 0.812 - 0s 87ms/step - loss: 0.9346 - accuracy: 0.82 - 0s 80ms/step - loss: 0.9195 - accuracy: 0.82 - 0s 76ms/step - loss: 0.9300 - accuracy: 0.81 - 1s 72ms/step - loss: 0.9509 - accuracy: 0.80 - 1s 69ms/step - loss: 0.9623 - accuracy: 0.80 - 1s 67ms/step - loss: 0.9595 - accuracy: 0.81 - 1s 65ms/step - loss: 0.9772 - accuracy: 0.80 - 1s 64ms/step - loss: 0.9585 - accuracy: 0.81 - 1s 63ms/step - loss: 0.9764 - accuracy: 0.81 - 1s 62ms/step - loss: 0.9751 - accuracy: 0.81 - 1s 61ms/step - loss: 0.9704 - accuracy: 0.81 - 1s 60ms/step - loss: 0.9723 - accuracy: 0.81 - 1s 60ms/step - loss: 0.9790 - accuracy: 0.81 - 1s 59ms/step - loss: 0.9862 - accuracy: 0.81 - 1s 58ms/step - loss: 0.9938 - accuracy: 0.81 - 1s 58ms/step - loss: 0.9893 - accuracy: 0.81 - 1s 58ms/step - loss: 0.9954 - accuracy: 0.81 - 1s 57ms/step - loss: 0.9850 - accuracy: 0.81 - 1s 57ms/step - loss: 0.9798 - accuracy: 0.82 - 1s 56ms/step - loss: 0.9883 - accuracy: 0.81 - 1s 56ms/step - loss: 0.9917 - accuracy: 0.81 - 1s 56ms/step - loss: 1.0025 - accuracy: 0.81 - 1s 56ms/step - loss: 1.0039 - accuracy: 0.81 - 1s 55ms/step - loss: 1.0030 - accuracy: 0.81 - 2s 55ms/step - loss: 0.9980 - accuracy: 0.81 - 2s 55ms/step - loss: 1.0009 - accuracy: 0.81 - 2s 55ms/step - loss: 0.9990 - accuracy: 0.81 - 2s 55ms/step - loss: 1.0004 - accuracy: 0.81 - 2s 54ms/step - loss: 0.9952 - accuracy: 0.81 - 2s 55ms/step - loss: 1.0008 - accuracy: 0.81 - 2s 54ms/step - loss: 1.0047 - accuracy: 0.81 - 2s 54ms/step - loss: 1.0022 - accuracy: 0.81 - 2s 54ms/step - loss: 0.9956 - accuracy: 0.81 - 2s 54ms/step - loss: 0.9969 - accuracy: 0.81 - 2s 54ms/step - loss: 0.9966 - accuracy: 0.81 - 2s 53ms/step - loss: 0.9969 - accuracy: 0.81 - 2s 53ms/step - loss: 0.9951 - accuracy: 0.81 - 2s 53ms/step - loss: 0.9994 - accuracy: 0.81 - 2s 53ms/step - loss: 0.9973 - accuracy: 0.81 - 2s 53ms/step - loss: 1.0012 - accuracy: 0.81 - 2s 53ms/step - loss: 0.9999 - accuracy: 0.81 - 2s 53ms/step - loss: 0.9950 - accuracy: 0.81 - 2s 53ms/step - loss: 0.9962 - accuracy: 0.81 - 2s 53ms/step - loss: 0.9957 - accuracy: 0.81 - 3s 53ms/step - loss: 0.9951 - accuracy: 0.81 - 3s 53ms/step - loss: 0.9973 - accuracy: 0.81 - 3s 52ms/step - loss: 0.9966 - accuracy: 0.81 - 3s 52ms/step - loss: 0.9977 - accuracy: 0.81 - 3s 52ms/step - loss: 0.9978 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0018 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0034 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0048 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0038 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0046 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0039 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0060 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0090 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0065 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0101 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0091 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0132 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0133 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0135 - accuracy: 0.81 - 3s 52ms/step - loss: 1.0147 - accuracy: 0.81 - 4s 52ms/step - loss: 1.0157 - accuracy: 0.81 - 4s 52ms/step - loss: 1.0145 - accuracy: 0.81 - 4s 52ms/step - loss: 1.0154 - accuracy: 0.81 - 4s 52ms/step - loss: 1.0163 - accuracy: 0.81 - 4s 52ms/step - loss: 1.0181 - accuracy: 0.81 - 4s 52ms/step - loss: 1.0221 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0208 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0229 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0211 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0209 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0189 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0189 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0196 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0212 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0220 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0241 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0241 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0243 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0249 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0257 - accuracy: 0.81 - 4s 51ms/step - loss: 1.0258 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0260 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0249 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0251 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0262 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0245 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0256 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0264 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0265 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0289 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0274 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0263 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0263 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0266 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0248 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0256 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0292 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0284 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0291 - accuracy: 0.81 - 5s 51ms/step - loss: 1.0304 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0303 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0314 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0333 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0318 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0330 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0343 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0345 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0339 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0342 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0334 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0341 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0335 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0334 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0329 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0332 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0337 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0345 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0341 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0349 - accuracy: 0.81 - 6s 51ms/step - loss: 1.0354 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0361 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0366 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0365 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0358 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0350 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0341 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0336 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0335 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0356 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0357 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0354 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0353 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0355 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0359 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0369 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0361 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0371 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0369 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0366 - accuracy: 0.81 - 7s 51ms/step - loss: 1.0357 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0352 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0350 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0365 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0372 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0363 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0348 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0345 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0332 - accuracy: 0.81 - 8s 51ms/step - loss: 1.0331 - accuracy: 0.81 - 8s 50ms/step - loss: 1.0317 - accuracy: 0.81 - 8s 53ms/step - loss: 1.0317 - accuracy: 0.8140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0316843682793295, 0.814]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.models.load_model('checkpoints/{}.hdf5'.format(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
