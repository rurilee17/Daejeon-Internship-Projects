{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1023 20:34:49.376404 18464 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# from keras.models import load_model\n",
    "from datetime import datetime\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfds.load(name='cifar10:3.*.*', split='train[:80%]', as_supervised=True)\n",
    "valid_data = tfds.load(name='cifar10:3.*.*', split='train[80%:]', as_supervised=True)\n",
    "test_data = tfds.load(name='cifar10:3.*.*', split='test', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                81930     \n",
      "=================================================================\n",
      "Total params: 82,954\n",
      "Trainable params: 82,890\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),  # 키워드 검색하기\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "train_data = train_data.map(scale).shuffle(10000).batch(64) # \n",
    "valid_data = valid_data.map(scale).batch(64)\n",
    "test_data = test_data.map(scale).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(   \n",
    "    'checkpoints/{}.hdf5'.format(dt),\n",
    "    monitor='val_acc',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_start(self, epoch, logs=None):\n",
    "        print('Epoch {} begins at {}: {}'.format(epoch, datetime.now().time(), logs))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {} ends at {}: {}'.format(epoch, datetime.now().time(), logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1023 20:34:53.209528 18464 deprecation.py:323] From C:\\Users\\Affinity\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ends at 20:36:56.914947: {'loss': 1.5966620832443237, 'acc': 0.45645, 'val_loss': 1.5100887732900632, 'val_acc': 0.4868}\n",
      "Epoch 1 ends at 20:39:04.615714: {'loss': 1.2489439393997193, 'acc': 0.569625, 'val_loss': 1.1727690157616975, 'val_acc': 0.5924}\n",
      "Epoch 2 ends at 20:41:12.605590: {'loss': 1.1461329936981202, 'acc': 0.602275, 'val_loss': 1.2461531055960686, 'val_acc': 0.575}\n",
      "Epoch 3 ends at 20:43:20.136050: {'loss': 1.089291981124878, 'acc': 0.6236, 'val_loss': 1.1592662820390836, 'val_acc': 0.6034}\n",
      "Epoch 4 ends at 20:45:27.747300: {'loss': 1.0528878829956054, 'acc': 0.6331, 'val_loss': 1.146697270262773, 'val_acc': 0.6141}\n",
      "Epoch 5 ends at 20:47:35.198957: {'loss': 1.017085502052307, 'acc': 0.64815, 'val_loss': 1.183781301140026, 'val_acc': 0.5978}\n",
      "Epoch 6 ends at 20:49:42.020394: {'loss': 0.9918185165405273, 'acc': 0.6543, 'val_loss': 1.203700229620478, 'val_acc': 0.5884}\n",
      "Epoch 7 ends at 20:51:49.084088: {'loss': 0.9869338397979737, 'acc': 0.65755, 'val_loss': 1.1358678413044876, 'val_acc': 0.6172}\n",
      "Epoch 8 ends at 20:53:56.767032: {'loss': 0.9618964607238769, 'acc': 0.66245, 'val_loss': 1.1693742559973601, 'val_acc': 0.6112}\n",
      "Epoch 9 ends at 20:56:04.282631: {'loss': 0.9446097882270813, 'acc': 0.669325, 'val_loss': 1.1903204747066376, 'val_acc': 0.6066}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    epochs=10,\n",
    "                    validation_data=valid_data,\n",
    "                    verbose=0,\n",
    "                    callbacks=[cp_callback, MyCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 108ms/step - loss: 1.0677 - acc: 0.73 - 0s 78ms/step - loss: 1.0869 - acc: 0.6719 - 0s 63ms/step - loss: 1.1522 - acc: 0.666 - 0s 52ms/step - loss: 1.0916 - acc: 0.671 - 0s 46ms/step - loss: 1.0768 - acc: 0.675 - 0s 42ms/step - loss: 1.0911 - acc: 0.666 - 0s 40ms/step - loss: 1.0912 - acc: 0.662 - 0s 38ms/step - loss: 1.0981 - acc: 0.658 - 0s 36ms/step - loss: 1.1073 - acc: 0.652 - 0s 35ms/step - loss: 1.0978 - acc: 0.656 - 0s 33ms/step - loss: 1.1070 - acc: 0.656 - 0s 32ms/step - loss: 1.1477 - acc: 0.644 - 0s 31ms/step - loss: 1.1373 - acc: 0.644 - 0s 30ms/step - loss: 1.1256 - acc: 0.641 - 0s 29ms/step - loss: 1.1308 - acc: 0.637 - 0s 29ms/step - loss: 1.1426 - acc: 0.634 - 0s 28ms/step - loss: 1.1492 - acc: 0.628 - 0s 27ms/step - loss: 1.1512 - acc: 0.625 - 1s 27ms/step - loss: 1.1576 - acc: 0.621 - 1s 27ms/step - loss: 1.1702 - acc: 0.618 - 1s 27ms/step - loss: 1.1535 - acc: 0.622 - 1s 26ms/step - loss: 1.1516 - acc: 0.625 - 1s 26ms/step - loss: 1.1538 - acc: 0.624 - 1s 26ms/step - loss: 1.1498 - acc: 0.625 - 1s 25ms/step - loss: 1.1434 - acc: 0.628 - 1s 25ms/step - loss: 1.1396 - acc: 0.628 - 1s 24ms/step - loss: 1.1405 - acc: 0.627 - 1s 24ms/step - loss: 1.1317 - acc: 0.628 - 1s 24ms/step - loss: 1.1407 - acc: 0.623 - 1s 24ms/step - loss: 1.1436 - acc: 0.622 - 1s 23ms/step - loss: 1.1374 - acc: 0.623 - 1s 23ms/step - loss: 1.1326 - acc: 0.624 - 1s 23ms/step - loss: 1.1408 - acc: 0.620 - 1s 23ms/step - loss: 1.1440 - acc: 0.619 - 1s 23ms/step - loss: 1.1399 - acc: 0.621 - 1s 23ms/step - loss: 1.1411 - acc: 0.621 - 1s 23ms/step - loss: 1.1445 - acc: 0.621 - 1s 23ms/step - loss: 1.1423 - acc: 0.621 - 1s 23ms/step - loss: 1.1385 - acc: 0.622 - 1s 22ms/step - loss: 1.1375 - acc: 0.622 - 1s 22ms/step - loss: 1.1385 - acc: 0.622 - 1s 22ms/step - loss: 1.1374 - acc: 0.622 - 1s 22ms/step - loss: 1.1435 - acc: 0.622 - 1s 23ms/step - loss: 1.1475 - acc: 0.620 - 1s 23ms/step - loss: 1.1408 - acc: 0.622 - 1s 23ms/step - loss: 1.1386 - acc: 0.622 - 1s 23ms/step - loss: 1.1426 - acc: 0.620 - 1s 22ms/step - loss: 1.1374 - acc: 0.621 - 1s 22ms/step - loss: 1.1405 - acc: 0.621 - 1s 22ms/step - loss: 1.1413 - acc: 0.621 - 1s 22ms/step - loss: 1.1416 - acc: 0.621 - 1s 22ms/step - loss: 1.1422 - acc: 0.621 - 1s 22ms/step - loss: 1.1544 - acc: 0.618 - 1s 22ms/step - loss: 1.1556 - acc: 0.617 - 1s 22ms/step - loss: 1.1542 - acc: 0.618 - 1s 22ms/step - loss: 1.1565 - acc: 0.617 - 1s 22ms/step - loss: 1.1501 - acc: 0.620 - 1s 22ms/step - loss: 1.1519 - acc: 0.619 - 1s 22ms/step - loss: 1.1497 - acc: 0.620 - 1s 22ms/step - loss: 1.1510 - acc: 0.620 - 1s 22ms/step - loss: 1.1533 - acc: 0.621 - 1s 22ms/step - loss: 1.1561 - acc: 0.619 - 1s 22ms/step - loss: 1.1565 - acc: 0.619 - 1s 22ms/step - loss: 1.1626 - acc: 0.617 - 1s 22ms/step - loss: 1.1651 - acc: 0.617 - 1s 22ms/step - loss: 1.1642 - acc: 0.617 - 1s 22ms/step - loss: 1.1646 - acc: 0.616 - 1s 22ms/step - loss: 1.1700 - acc: 0.614 - 1s 22ms/step - loss: 1.1689 - acc: 0.613 - 2s 22ms/step - loss: 1.1644 - acc: 0.614 - 2s 22ms/step - loss: 1.1672 - acc: 0.614 - 2s 22ms/step - loss: 1.1697 - acc: 0.613 - 2s 22ms/step - loss: 1.1713 - acc: 0.613 - 2s 22ms/step - loss: 1.1708 - acc: 0.613 - 2s 22ms/step - loss: 1.1706 - acc: 0.612 - 2s 22ms/step - loss: 1.1668 - acc: 0.614 - 2s 22ms/step - loss: 1.1670 - acc: 0.613 - 2s 21ms/step - loss: 1.1653 - acc: 0.614 - 2s 21ms/step - loss: 1.1637 - acc: 0.614 - 2s 21ms/step - loss: 1.1608 - acc: 0.615 - 2s 21ms/step - loss: 1.1611 - acc: 0.614 - 2s 21ms/step - loss: 1.1612 - acc: 0.614 - 2s 21ms/step - loss: 1.1594 - acc: 0.615 - 2s 21ms/step - loss: 1.1604 - acc: 0.615 - 2s 21ms/step - loss: 1.1585 - acc: 0.615 - 2s 21ms/step - loss: 1.1577 - acc: 0.615 - 2s 21ms/step - loss: 1.1580 - acc: 0.615 - 2s 21ms/step - loss: 1.1567 - acc: 0.615 - 2s 21ms/step - loss: 1.1584 - acc: 0.613 - 2s 21ms/step - loss: 1.1634 - acc: 0.612 - 2s 21ms/step - loss: 1.1657 - acc: 0.612 - 2s 21ms/step - loss: 1.1640 - acc: 0.612 - 2s 21ms/step - loss: 1.1617 - acc: 0.613 - 2s 21ms/step - loss: 1.1627 - acc: 0.613 - 2s 21ms/step - loss: 1.1680 - acc: 0.612 - 2s 21ms/step - loss: 1.1686 - acc: 0.611 - 2s 21ms/step - loss: 1.1705 - acc: 0.610 - 2s 21ms/step - loss: 1.1710 - acc: 0.610 - 2s 21ms/step - loss: 1.1734 - acc: 0.609 - 2s 21ms/step - loss: 1.1710 - acc: 0.609 - 2s 21ms/step - loss: 1.1690 - acc: 0.609 - 2s 21ms/step - loss: 1.1677 - acc: 0.609 - 2s 21ms/step - loss: 1.1677 - acc: 0.609 - 2s 21ms/step - loss: 1.1696 - acc: 0.608 - 2s 21ms/step - loss: 1.1696 - acc: 0.608 - 2s 21ms/step - loss: 1.1704 - acc: 0.608 - 2s 21ms/step - loss: 1.1700 - acc: 0.608 - 2s 21ms/step - loss: 1.1682 - acc: 0.608 - 2s 21ms/step - loss: 1.1676 - acc: 0.609 - 2s 21ms/step - loss: 1.1707 - acc: 0.608 - 2s 21ms/step - loss: 1.1698 - acc: 0.608 - 2s 21ms/step - loss: 1.1707 - acc: 0.608 - 2s 21ms/step - loss: 1.1730 - acc: 0.608 - 2s 21ms/step - loss: 1.1723 - acc: 0.607 - 2s 21ms/step - loss: 1.1724 - acc: 0.607 - 2s 21ms/step - loss: 1.1724 - acc: 0.607 - 2s 21ms/step - loss: 1.1740 - acc: 0.607 - 3s 21ms/step - loss: 1.1737 - acc: 0.607 - 3s 21ms/step - loss: 1.1734 - acc: 0.608 - 3s 21ms/step - loss: 1.1740 - acc: 0.608 - 3s 21ms/step - loss: 1.1752 - acc: 0.607 - 3s 21ms/step - loss: 1.1745 - acc: 0.607 - 3s 21ms/step - loss: 1.1735 - acc: 0.607 - 3s 21ms/step - loss: 1.1735 - acc: 0.607 - 3s 21ms/step - loss: 1.1722 - acc: 0.608 - 3s 21ms/step - loss: 1.1732 - acc: 0.607 - 3s 21ms/step - loss: 1.1745 - acc: 0.607 - 3s 21ms/step - loss: 1.1751 - acc: 0.607 - 3s 21ms/step - loss: 1.1739 - acc: 0.607 - 3s 21ms/step - loss: 1.1720 - acc: 0.608 - 3s 21ms/step - loss: 1.1719 - acc: 0.607 - 3s 21ms/step - loss: 1.1713 - acc: 0.607 - 3s 21ms/step - loss: 1.1721 - acc: 0.607 - 3s 21ms/step - loss: 1.1708 - acc: 0.608 - 3s 21ms/step - loss: 1.1696 - acc: 0.608 - 3s 21ms/step - loss: 1.1711 - acc: 0.608 - 3s 21ms/step - loss: 1.1740 - acc: 0.607 - 3s 21ms/step - loss: 1.1744 - acc: 0.606 - 3s 21ms/step - loss: 1.1759 - acc: 0.607 - 3s 21ms/step - loss: 1.1756 - acc: 0.606 - 3s 21ms/step - loss: 1.1756 - acc: 0.606 - 3s 21ms/step - loss: 1.1756 - acc: 0.606 - 3s 21ms/step - loss: 1.1751 - acc: 0.606 - 3s 21ms/step - loss: 1.1743 - acc: 0.606 - 3s 21ms/step - loss: 1.1736 - acc: 0.606 - 3s 21ms/step - loss: 1.1741 - acc: 0.606 - 3s 21ms/step - loss: 1.1747 - acc: 0.606 - 3s 21ms/step - loss: 1.1741 - acc: 0.606 - 3s 21ms/step - loss: 1.1743 - acc: 0.607 - 3s 21ms/step - loss: 1.1748 - acc: 0.607 - 3s 21ms/step - loss: 1.1742 - acc: 0.606 - 3s 21ms/step - loss: 1.1738 - acc: 0.607 - 3s 21ms/step - loss: 1.1739 - acc: 0.607 - 3s 21ms/step - loss: 1.1738 - acc: 0.606 - 3s 21ms/step - loss: 1.1746 - acc: 0.606 - 3s 21ms/step - loss: 1.1754 - acc: 0.606 - 3s 21ms/step - loss: 1.1759 - acc: 0.606 - 3s 21ms/step - loss: 1.1759 - acc: 0.6060"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1759216102065555, 0.606]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.models.load_model('checkpoints/best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                81930     \n",
      "=================================================================\n",
      "Total params: 82,954\n",
      "Trainable params: 82,890\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 229ms/step - loss: 1.4995 - acc: 0.50 - 0s 127ms/step - loss: 1.4675 - acc: 0.56 - 0s 93ms/step - loss: 1.5331 - acc: 0.5625 - 0s 76ms/step - loss: 1.4625 - acc: 0.574 - 0s 64ms/step - loss: 1.4089 - acc: 0.578 - 0s 58ms/step - loss: 1.3938 - acc: 0.575 - 0s 53ms/step - loss: 1.3986 - acc: 0.567 - 0s 49ms/step - loss: 1.4066 - acc: 0.562 - 0s 46ms/step - loss: 1.4087 - acc: 0.560 - 0s 44ms/step - loss: 1.4016 - acc: 0.560 - 0s 42ms/step - loss: 1.4158 - acc: 0.558 - 0s 40ms/step - loss: 1.4410 - acc: 0.544 - 0s 38ms/step - loss: 1.4386 - acc: 0.544 - 1s 36ms/step - loss: 1.4555 - acc: 0.539 - 1s 36ms/step - loss: 1.4599 - acc: 0.539 - 1s 35ms/step - loss: 1.4672 - acc: 0.537 - 1s 34ms/step - loss: 1.4756 - acc: 0.531 - 1s 33ms/step - loss: 1.4793 - acc: 0.526 - 1s 32ms/step - loss: 1.4836 - acc: 0.524 - 1s 32ms/step - loss: 1.4974 - acc: 0.517 - 1s 31ms/step - loss: 1.4655 - acc: 0.526 - 1s 31ms/step - loss: 1.4636 - acc: 0.526 - 1s 31ms/step - loss: 1.4568 - acc: 0.527 - 1s 30ms/step - loss: 1.4631 - acc: 0.526 - 1s 30ms/step - loss: 1.4622 - acc: 0.525 - 1s 30ms/step - loss: 1.4550 - acc: 0.524 - 1s 29ms/step - loss: 1.4579 - acc: 0.524 - 1s 29ms/step - loss: 1.4523 - acc: 0.527 - 1s 29ms/step - loss: 1.4552 - acc: 0.522 - 1s 29ms/step - loss: 1.4636 - acc: 0.520 - 1s 29ms/step - loss: 1.4568 - acc: 0.521 - 1s 29ms/step - loss: 1.4534 - acc: 0.523 - 1s 29ms/step - loss: 1.4621 - acc: 0.521 - 1s 29ms/step - loss: 1.4636 - acc: 0.520 - 1s 28ms/step - loss: 1.4592 - acc: 0.524 - 1s 28ms/step - loss: 1.4574 - acc: 0.523 - 1s 28ms/step - loss: 1.4619 - acc: 0.522 - 1s 28ms/step - loss: 1.4629 - acc: 0.522 - 1s 28ms/step - loss: 1.4606 - acc: 0.522 - 1s 27ms/step - loss: 1.4623 - acc: 0.522 - 1s 27ms/step - loss: 1.4611 - acc: 0.521 - 1s 27ms/step - loss: 1.4537 - acc: 0.522 - 1s 27ms/step - loss: 1.4542 - acc: 0.522 - 1s 26ms/step - loss: 1.4610 - acc: 0.520 - 1s 26ms/step - loss: 1.4555 - acc: 0.521 - 1s 26ms/step - loss: 1.4550 - acc: 0.522 - 1s 26ms/step - loss: 1.4554 - acc: 0.521 - 1s 26ms/step - loss: 1.4495 - acc: 0.522 - 1s 26ms/step - loss: 1.4491 - acc: 0.522 - 1s 26ms/step - loss: 1.4480 - acc: 0.522 - 1s 26ms/step - loss: 1.4475 - acc: 0.520 - 1s 26ms/step - loss: 1.4480 - acc: 0.521 - 1s 26ms/step - loss: 1.4573 - acc: 0.520 - 1s 26ms/step - loss: 1.4625 - acc: 0.517 - 1s 26ms/step - loss: 1.4595 - acc: 0.517 - 1s 26ms/step - loss: 1.4591 - acc: 0.517 - 1s 25ms/step - loss: 1.4505 - acc: 0.517 - 1s 25ms/step - loss: 1.4493 - acc: 0.517 - 1s 25ms/step - loss: 1.4445 - acc: 0.519 - 2s 25ms/step - loss: 1.4455 - acc: 0.519 - 2s 25ms/step - loss: 1.4455 - acc: 0.519 - 2s 25ms/step - loss: 1.4475 - acc: 0.518 - 2s 25ms/step - loss: 1.4445 - acc: 0.519 - 2s 25ms/step - loss: 1.4501 - acc: 0.518 - 2s 25ms/step - loss: 1.4517 - acc: 0.518 - 2s 25ms/step - loss: 1.4514 - acc: 0.518 - 2s 25ms/step - loss: 1.4569 - acc: 0.517 - 2s 25ms/step - loss: 1.4583 - acc: 0.516 - 2s 25ms/step - loss: 1.4586 - acc: 0.516 - 2s 25ms/step - loss: 1.4579 - acc: 0.516 - 2s 24ms/step - loss: 1.4595 - acc: 0.516 - 2s 24ms/step - loss: 1.4621 - acc: 0.516 - 2s 24ms/step - loss: 1.4634 - acc: 0.517 - 2s 24ms/step - loss: 1.4634 - acc: 0.516 - 2s 24ms/step - loss: 1.4642 - acc: 0.516 - 2s 24ms/step - loss: 1.4625 - acc: 0.516 - 2s 24ms/step - loss: 1.4640 - acc: 0.515 - 2s 24ms/step - loss: 1.4608 - acc: 0.515 - 2s 24ms/step - loss: 1.4592 - acc: 0.515 - 2s 24ms/step - loss: 1.4567 - acc: 0.515 - 2s 24ms/step - loss: 1.4604 - acc: 0.514 - 2s 24ms/step - loss: 1.4601 - acc: 0.514 - 2s 24ms/step - loss: 1.4605 - acc: 0.513 - 2s 24ms/step - loss: 1.4619 - acc: 0.514 - 2s 24ms/step - loss: 1.4623 - acc: 0.515 - 2s 24ms/step - loss: 1.4619 - acc: 0.513 - 2s 24ms/step - loss: 1.4598 - acc: 0.514 - 2s 24ms/step - loss: 1.4585 - acc: 0.515 - 2s 24ms/step - loss: 1.4616 - acc: 0.514 - 2s 24ms/step - loss: 1.4659 - acc: 0.514 - 2s 24ms/step - loss: 1.4643 - acc: 0.514 - 2s 24ms/step - loss: 1.4634 - acc: 0.514 - 2s 24ms/step - loss: 1.4634 - acc: 0.514 - 2s 24ms/step - loss: 1.4639 - acc: 0.514 - 2s 23ms/step - loss: 1.4692 - acc: 0.513 - 2s 23ms/step - loss: 1.4693 - acc: 0.513 - 2s 23ms/step - loss: 1.4724 - acc: 0.512 - 2s 23ms/step - loss: 1.4736 - acc: 0.512 - 2s 23ms/step - loss: 1.4738 - acc: 0.512 - 2s 23ms/step - loss: 1.4736 - acc: 0.512 - 2s 23ms/step - loss: 1.4713 - acc: 0.513 - 2s 23ms/step - loss: 1.4677 - acc: 0.513 - 2s 23ms/step - loss: 1.4699 - acc: 0.512 - 2s 23ms/step - loss: 1.4700 - acc: 0.513 - 2s 23ms/step - loss: 1.4714 - acc: 0.513 - 2s 23ms/step - loss: 1.4720 - acc: 0.512 - 2s 23ms/step - loss: 1.4726 - acc: 0.511 - 3s 23ms/step - loss: 1.4739 - acc: 0.512 - 3s 23ms/step - loss: 1.4737 - acc: 0.512 - 3s 23ms/step - loss: 1.4768 - acc: 0.511 - 3s 23ms/step - loss: 1.4762 - acc: 0.511 - 3s 23ms/step - loss: 1.4772 - acc: 0.511 - 3s 23ms/step - loss: 1.4794 - acc: 0.510 - 3s 23ms/step - loss: 1.4788 - acc: 0.511 - 3s 23ms/step - loss: 1.4803 - acc: 0.511 - 3s 23ms/step - loss: 1.4794 - acc: 0.512 - 3s 23ms/step - loss: 1.4785 - acc: 0.512 - 3s 23ms/step - loss: 1.4785 - acc: 0.513 - 3s 23ms/step - loss: 1.4797 - acc: 0.512 - 3s 23ms/step - loss: 1.4806 - acc: 0.512 - 3s 23ms/step - loss: 1.4815 - acc: 0.512 - 3s 23ms/step - loss: 1.4812 - acc: 0.512 - 3s 23ms/step - loss: 1.4801 - acc: 0.512 - 3s 23ms/step - loss: 1.4803 - acc: 0.512 - 3s 23ms/step - loss: 1.4800 - acc: 0.512 - 3s 23ms/step - loss: 1.4794 - acc: 0.512 - 3s 23ms/step - loss: 1.4819 - acc: 0.512 - 3s 23ms/step - loss: 1.4832 - acc: 0.512 - 3s 23ms/step - loss: 1.4827 - acc: 0.511 - 3s 23ms/step - loss: 1.4789 - acc: 0.511 - 3s 23ms/step - loss: 1.4791 - acc: 0.511 - 3s 23ms/step - loss: 1.4789 - acc: 0.511 - 3s 23ms/step - loss: 1.4791 - acc: 0.511 - 3s 23ms/step - loss: 1.4780 - acc: 0.512 - 3s 23ms/step - loss: 1.4758 - acc: 0.512 - 3s 22ms/step - loss: 1.4761 - acc: 0.512 - 3s 22ms/step - loss: 1.4768 - acc: 0.512 - 3s 22ms/step - loss: 1.4772 - acc: 0.512 - 3s 22ms/step - loss: 1.4787 - acc: 0.512 - 3s 22ms/step - loss: 1.4777 - acc: 0.512 - 3s 22ms/step - loss: 1.4794 - acc: 0.512 - 3s 22ms/step - loss: 1.4789 - acc: 0.511 - 3s 22ms/step - loss: 1.4778 - acc: 0.512 - 3s 22ms/step - loss: 1.4787 - acc: 0.512 - 3s 22ms/step - loss: 1.4779 - acc: 0.512 - 3s 22ms/step - loss: 1.4785 - acc: 0.511 - 3s 22ms/step - loss: 1.4796 - acc: 0.511 - 3s 22ms/step - loss: 1.4792 - acc: 0.511 - 3s 22ms/step - loss: 1.4810 - acc: 0.511 - 3s 22ms/step - loss: 1.4811 - acc: 0.511 - 3s 22ms/step - loss: 1.4795 - acc: 0.511 - 3s 22ms/step - loss: 1.4780 - acc: 0.512 - 3s 22ms/step - loss: 1.4780 - acc: 0.512 - 3s 22ms/step - loss: 1.4791 - acc: 0.511 - 3s 22ms/step - loss: 1.4795 - acc: 0.511 - 3s 22ms/step - loss: 1.4799 - acc: 0.511 - 4s 23ms/step - loss: 1.4784 - acc: 0.511 - 4s 23ms/step - loss: 1.4784 - acc: 0.5119"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4784323979335225, 0.5119]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 433ms/step - loss: 1.9417 - acc: 0.48 - 0s 229ms/step - loss: 1.5641 - acc: 0.56 - 0s 159ms/step - loss: 1.5216 - acc: 0.55 - 0s 123ms/step - loss: 1.4327 - acc: 0.55 - 1s 103ms/step - loss: 1.4138 - acc: 0.55 - 1s 90ms/step - loss: 1.4655 - acc: 0.5339 - 1s 80ms/step - loss: 1.4854 - acc: 0.524 - 1s 72ms/step - loss: 1.4733 - acc: 0.525 - 1s 66ms/step - loss: 1.4693 - acc: 0.517 - 1s 61ms/step - loss: 1.4625 - acc: 0.517 - 1s 57ms/step - loss: 1.4947 - acc: 0.504 - 1s 55ms/step - loss: 1.4954 - acc: 0.506 - 1s 52ms/step - loss: 1.4911 - acc: 0.509 - 1s 50ms/step - loss: 1.5123 - acc: 0.503 - 1s 48ms/step - loss: 1.5227 - acc: 0.497 - 1s 47ms/step - loss: 1.5225 - acc: 0.499 - 1s 46ms/step - loss: 1.5072 - acc: 0.498 - 1s 45ms/step - loss: 1.5206 - acc: 0.496 - 1s 44ms/step - loss: 1.5086 - acc: 0.500 - 1s 43ms/step - loss: 1.5045 - acc: 0.498 - 1s 42ms/step - loss: 1.5103 - acc: 0.499 - 1s 41ms/step - loss: 1.5014 - acc: 0.503 - 1s 40ms/step - loss: 1.4913 - acc: 0.507 - 1s 39ms/step - loss: 1.4939 - acc: 0.505 - 1s 38ms/step - loss: 1.5034 - acc: 0.501 - 1s 38ms/step - loss: 1.5051 - acc: 0.500 - 1s 37ms/step - loss: 1.5061 - acc: 0.501 - 1s 36ms/step - loss: 1.5050 - acc: 0.498 - 1s 36ms/step - loss: 1.4956 - acc: 0.502 - 1s 35ms/step - loss: 1.4944 - acc: 0.502 - 1s 35ms/step - loss: 1.4964 - acc: 0.500 - 1s 34ms/step - loss: 1.4857 - acc: 0.503 - 1s 34ms/step - loss: 1.5010 - acc: 0.499 - 1s 34ms/step - loss: 1.5074 - acc: 0.496 - 1s 33ms/step - loss: 1.5064 - acc: 0.496 - 1s 33ms/step - loss: 1.4981 - acc: 0.497 - 1s 33ms/step - loss: 1.5153 - acc: 0.494 - 1s 32ms/step - loss: 1.5123 - acc: 0.495 - 1s 32ms/step - loss: 1.5013 - acc: 0.498 - 1s 32ms/step - loss: 1.5059 - acc: 0.498 - 1s 32ms/step - loss: 1.5075 - acc: 0.496 - 1s 31ms/step - loss: 1.5137 - acc: 0.497 - 1s 31ms/step - loss: 1.5085 - acc: 0.498 - 1s 31ms/step - loss: 1.5000 - acc: 0.502 - 1s 30ms/step - loss: 1.4953 - acc: 0.503 - 1s 30ms/step - loss: 1.4991 - acc: 0.502 - 1s 30ms/step - loss: 1.4989 - acc: 0.504 - 1s 30ms/step - loss: 1.4945 - acc: 0.504 - 1s 30ms/step - loss: 1.4936 - acc: 0.503 - 1s 29ms/step - loss: 1.4898 - acc: 0.505 - 1s 29ms/step - loss: 1.4931 - acc: 0.504 - 2s 29ms/step - loss: 1.4944 - acc: 0.503 - 2s 29ms/step - loss: 1.4948 - acc: 0.504 - 2s 29ms/step - loss: 1.4983 - acc: 0.502 - 2s 29ms/step - loss: 1.4970 - acc: 0.503 - 2s 29ms/step - loss: 1.4992 - acc: 0.503 - 2s 28ms/step - loss: 1.4970 - acc: 0.502 - 2s 28ms/step - loss: 1.4987 - acc: 0.502 - 2s 28ms/step - loss: 1.4953 - acc: 0.501 - 2s 28ms/step - loss: 1.4949 - acc: 0.502 - 2s 28ms/step - loss: 1.5004 - acc: 0.500 - 2s 28ms/step - loss: 1.5030 - acc: 0.500 - 2s 28ms/step - loss: 1.4956 - acc: 0.501 - 2s 28ms/step - loss: 1.4951 - acc: 0.502 - 2s 28ms/step - loss: 1.4935 - acc: 0.502 - 2s 28ms/step - loss: 1.4915 - acc: 0.503 - 2s 28ms/step - loss: 1.4949 - acc: 0.502 - 2s 28ms/step - loss: 1.4948 - acc: 0.502 - 2s 28ms/step - loss: 1.4956 - acc: 0.501 - 2s 27ms/step - loss: 1.4974 - acc: 0.501 - 2s 27ms/step - loss: 1.5016 - acc: 0.500 - 2s 27ms/step - loss: 1.5011 - acc: 0.501 - 2s 27ms/step - loss: 1.5030 - acc: 0.501 - 2s 27ms/step - loss: 1.5087 - acc: 0.499 - 2s 27ms/step - loss: 1.5039 - acc: 0.500 - 2s 27ms/step - loss: 1.5002 - acc: 0.502 - 2s 27ms/step - loss: 1.4993 - acc: 0.502 - 2s 27ms/step - loss: 1.4954 - acc: 0.503 - 2s 27ms/step - loss: 1.4955 - acc: 0.503 - 2s 27ms/step - loss: 1.4979 - acc: 0.502 - 2s 27ms/step - loss: 1.4954 - acc: 0.504 - 2s 27ms/step - loss: 1.4952 - acc: 0.504 - 2s 27ms/step - loss: 1.4941 - acc: 0.503 - 2s 26ms/step - loss: 1.4938 - acc: 0.503 - 2s 26ms/step - loss: 1.4942 - acc: 0.503 - 2s 26ms/step - loss: 1.4904 - acc: 0.504 - 2s 26ms/step - loss: 1.4874 - acc: 0.505 - 2s 26ms/step - loss: 1.4912 - acc: 0.503 - 2s 26ms/step - loss: 1.4909 - acc: 0.503 - 2s 26ms/step - loss: 1.4883 - acc: 0.504 - 2s 26ms/step - loss: 1.4870 - acc: 0.504 - 2s 26ms/step - loss: 1.4886 - acc: 0.504 - 2s 26ms/step - loss: 1.4894 - acc: 0.503 - 2s 26ms/step - loss: 1.4872 - acc: 0.504 - 2s 26ms/step - loss: 1.4887 - acc: 0.503 - 2s 26ms/step - loss: 1.4912 - acc: 0.503 - 2s 26ms/step - loss: 1.4933 - acc: 0.502 - 3s 26ms/step - loss: 1.4899 - acc: 0.502 - 3s 26ms/step - loss: 1.4919 - acc: 0.501 - 3s 26ms/step - loss: 1.4950 - acc: 0.501 - 3s 25ms/step - loss: 1.4945 - acc: 0.500 - 3s 25ms/step - loss: 1.4959 - acc: 0.500 - 3s 25ms/step - loss: 1.4954 - acc: 0.500 - 3s 25ms/step - loss: 1.4944 - acc: 0.500 - 3s 25ms/step - loss: 1.4937 - acc: 0.501 - 3s 25ms/step - loss: 1.4919 - acc: 0.501 - 3s 25ms/step - loss: 1.4918 - acc: 0.502 - 3s 25ms/step - loss: 1.4925 - acc: 0.502 - 3s 25ms/step - loss: 1.4965 - acc: 0.501 - 3s 25ms/step - loss: 1.4970 - acc: 0.501 - 3s 25ms/step - loss: 1.4972 - acc: 0.502 - 3s 25ms/step - loss: 1.4984 - acc: 0.502 - 3s 25ms/step - loss: 1.5010 - acc: 0.501 - 3s 25ms/step - loss: 1.5002 - acc: 0.502 - 3s 25ms/step - loss: 1.5022 - acc: 0.502 - 3s 25ms/step - loss: 1.5009 - acc: 0.503 - 3s 25ms/step - loss: 1.5016 - acc: 0.502 - 3s 25ms/step - loss: 1.5020 - acc: 0.502 - 3s 25ms/step - loss: 1.5037 - acc: 0.503 - 3s 25ms/step - loss: 1.5019 - acc: 0.503 - 3s 25ms/step - loss: 1.5010 - acc: 0.503 - 3s 25ms/step - loss: 1.5002 - acc: 0.503 - 3s 25ms/step - loss: 1.4988 - acc: 0.504 - 3s 25ms/step - loss: 1.5006 - acc: 0.503 - 3s 25ms/step - loss: 1.5006 - acc: 0.504 - 3s 25ms/step - loss: 1.4999 - acc: 0.504 - 3s 25ms/step - loss: 1.4986 - acc: 0.504 - 3s 25ms/step - loss: 1.4992 - acc: 0.504 - 3s 25ms/step - loss: 1.4988 - acc: 0.505 - 3s 24ms/step - loss: 1.4999 - acc: 0.504 - 3s 24ms/step - loss: 1.4980 - acc: 0.505 - 3s 24ms/step - loss: 1.4971 - acc: 0.505 - 3s 24ms/step - loss: 1.4956 - acc: 0.505 - 3s 24ms/step - loss: 1.4954 - acc: 0.505 - 3s 24ms/step - loss: 1.4957 - acc: 0.504 - 3s 24ms/step - loss: 1.4941 - acc: 0.505 - 3s 24ms/step - loss: 1.4950 - acc: 0.504 - 3s 24ms/step - loss: 1.4947 - acc: 0.504 - 3s 24ms/step - loss: 1.4933 - acc: 0.504 - 3s 24ms/step - loss: 1.4932 - acc: 0.504 - 3s 24ms/step - loss: 1.4905 - acc: 0.505 - 3s 24ms/step - loss: 1.4901 - acc: 0.504 - 3s 24ms/step - loss: 1.4902 - acc: 0.505 - 3s 24ms/step - loss: 1.4896 - acc: 0.505 - 3s 24ms/step - loss: 1.4878 - acc: 0.505 - 3s 24ms/step - loss: 1.4853 - acc: 0.506 - 4s 24ms/step - loss: 1.4851 - acc: 0.506 - 4s 24ms/step - loss: 1.4835 - acc: 0.506 - 4s 24ms/step - loss: 1.4820 - acc: 0.507 - 4s 24ms/step - loss: 1.4830 - acc: 0.507 - 4s 24ms/step - loss: 1.4858 - acc: 0.506 - 4s 24ms/step - loss: 1.4857 - acc: 0.506 - 4s 24ms/step - loss: 1.4832 - acc: 0.506 - 4s 24ms/step - loss: 1.4832 - acc: 0.507 - 4s 24ms/step - loss: 1.4821 - acc: 0.507 - 4s 24ms/step - loss: 1.4815 - acc: 0.508 - 4s 24ms/step - loss: 1.4847 - acc: 0.508 - 4s 24ms/step - loss: 1.4847 - acc: 0.5080"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4847465670032867, 0.508]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
