{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1024 14:43:12.257216 14744 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# from keras.models import load_model\n",
    "from datetime import datetime\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfds.load(name='cifar10:3.*.*', split='train[:80%]', as_supervised=True)\n",
    "valid_data = tfds.load(name='cifar10:3.*.*', split='train[80%:]', as_supervised=True)\n",
    "test_data = tfds.load(name='cifar10:3.*.*', split='test', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 32)          18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 101,866\n",
      "Trainable params: 101,418\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),  # Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나이다\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나이다\n",
    "    tf.keras.layers.MaxPooling2D(),    \n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나이다\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "train_data = train_data.map(scale).shuffle(50000).batch(64) # \n",
    "valid_data = valid_data.map(scale).batch(64)\n",
    "test_data = test_data.map(scale).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(   \n",
    "    'checkpoints/{}.hdf5'.format(dt),\n",
    "    monitor='val_acc',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_start(self, epoch, logs=None):\n",
    "        print('Epoch {} begins at {}: {}'.format(epoch, datetime.now().time(), logs))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {} ends at {}: {}'.format(epoch, datetime.now().time(), logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1024 14:44:13.171307 14744 deprecation.py:323] From C:\\Users\\Affinity\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ends at 14:50:47.593855: {'loss': 1.7538973316192628, 'acc': 0.423425, 'val_loss': 1.4163781693027278, 'val_acc': 0.4933}\n",
      "Epoch 1 ends at 14:57:26.374454: {'loss': 1.1922735773086548, 'acc': 0.580475, 'val_loss': 1.1919147076120802, 'val_acc': 0.583}\n",
      "Epoch 2 ends at 15:04:07.705772: {'loss': 1.0225573006629944, 'acc': 0.64025, 'val_loss': 1.230954049119524, 'val_acc': 0.5635}\n",
      "Epoch 3 ends at 15:10:43.338337: {'loss': 0.9361002067565918, 'acc': 0.66855, 'val_loss': 1.0627076986489024, 'val_acc': 0.6219}\n",
      "Epoch 4 ends at 15:17:18.990613: {'loss': 0.8761671788692474, 'acc': 0.6927, 'val_loss': 0.965434617677312, 'val_acc': 0.6594}\n",
      "Epoch 5 ends at 15:24:00.291806: {'loss': 0.8228369772911072, 'acc': 0.7107, 'val_loss': 0.9002092731226782, 'val_acc': 0.6809}\n",
      "Epoch 6 ends at 15:30:38.501201: {'loss': 0.7946673872947693, 'acc': 0.72055, 'val_loss': 0.968529917631939, 'val_acc': 0.6654}\n",
      "Epoch 7 ends at 15:37:23.631113: {'loss': 0.7666522501468659, 'acc': 0.7294, 'val_loss': 1.0548914010357704, 'val_acc': 0.6414}\n",
      "Epoch 8 ends at 15:44:09.546922: {'loss': 0.7387273489952088, 'acc': 0.742125, 'val_loss': 1.0230157538584084, 'val_acc': 0.6482}\n",
      "Epoch 9 ends at 15:50:54.413540: {'loss': 0.7152809860229492, 'acc': 0.74905, 'val_loss': 0.9085101060047271, 'val_acc': 0.6873}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    epochs=10,\n",
    "                    validation_data=valid_data,\n",
    "                    verbose=0,\n",
    "                    callbacks=[cp_callback, MyCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 58ms/step - loss: 0.7984 - acc: 0.718 - 0s 56ms/step - loss: 0.9168 - acc: 0.695 - 0s 57ms/step - loss: 0.9136 - acc: 0.677 - 0s 56ms/step - loss: 0.8498 - acc: 0.695 - 0s 55ms/step - loss: 0.8475 - acc: 0.700 - 0s 55ms/step - loss: 0.8406 - acc: 0.703 - 0s 55ms/step - loss: 0.8446 - acc: 0.700 - 0s 54ms/step - loss: 0.8485 - acc: 0.701 - 0s 54ms/step - loss: 0.8446 - acc: 0.701 - 1s 54ms/step - loss: 0.8585 - acc: 0.700 - 1s 55ms/step - loss: 0.8619 - acc: 0.694 - 1s 55ms/step - loss: 0.8803 - acc: 0.692 - 1s 55ms/step - loss: 0.8682 - acc: 0.694 - 1s 55ms/step - loss: 0.8641 - acc: 0.695 - 1s 55ms/step - loss: 0.8579 - acc: 0.700 - 1s 55ms/step - loss: 0.8707 - acc: 0.696 - 1s 55ms/step - loss: 0.8670 - acc: 0.696 - 1s 55ms/step - loss: 0.8754 - acc: 0.691 - 1s 56ms/step - loss: 0.8785 - acc: 0.689 - 1s 56ms/step - loss: 0.8940 - acc: 0.683 - 1s 56ms/step - loss: 0.8814 - acc: 0.687 - 1s 56ms/step - loss: 0.8799 - acc: 0.691 - 1s 56ms/step - loss: 0.8787 - acc: 0.692 - 1s 56ms/step - loss: 0.8816 - acc: 0.691 - 1s 56ms/step - loss: 0.8743 - acc: 0.695 - 1s 56ms/step - loss: 0.8724 - acc: 0.695 - 2s 56ms/step - loss: 0.8700 - acc: 0.695 - 2s 56ms/step - loss: 0.8632 - acc: 0.696 - 2s 56ms/step - loss: 0.8661 - acc: 0.696 - 2s 56ms/step - loss: 0.8723 - acc: 0.695 - 2s 56ms/step - loss: 0.8732 - acc: 0.695 - 2s 56ms/step - loss: 0.8701 - acc: 0.696 - 2s 56ms/step - loss: 0.8749 - acc: 0.695 - 2s 56ms/step - loss: 0.8749 - acc: 0.695 - 2s 56ms/step - loss: 0.8736 - acc: 0.696 - 2s 56ms/step - loss: 0.8742 - acc: 0.694 - 2s 56ms/step - loss: 0.8702 - acc: 0.695 - 2s 56ms/step - loss: 0.8752 - acc: 0.692 - 2s 56ms/step - loss: 0.8750 - acc: 0.691 - 2s 56ms/step - loss: 0.8747 - acc: 0.691 - 2s 56ms/step - loss: 0.8776 - acc: 0.691 - 2s 56ms/step - loss: 0.8785 - acc: 0.690 - 2s 56ms/step - loss: 0.8812 - acc: 0.690 - 2s 56ms/step - loss: 0.8860 - acc: 0.689 - 3s 56ms/step - loss: 0.8781 - acc: 0.691 - 3s 56ms/step - loss: 0.8770 - acc: 0.690 - 3s 55ms/step - loss: 0.8771 - acc: 0.690 - 3s 55ms/step - loss: 0.8763 - acc: 0.692 - 3s 55ms/step - loss: 0.8780 - acc: 0.691 - 3s 55ms/step - loss: 0.8812 - acc: 0.689 - 3s 55ms/step - loss: 0.8836 - acc: 0.688 - 3s 55ms/step - loss: 0.8842 - acc: 0.687 - 3s 55ms/step - loss: 0.8888 - acc: 0.684 - 3s 55ms/step - loss: 0.8913 - acc: 0.683 - 3s 55ms/step - loss: 0.8873 - acc: 0.685 - 3s 55ms/step - loss: 0.8885 - acc: 0.685 - 3s 55ms/step - loss: 0.8858 - acc: 0.686 - 3s 55ms/step - loss: 0.8844 - acc: 0.687 - 3s 55ms/step - loss: 0.8829 - acc: 0.688 - 3s 56ms/step - loss: 0.8852 - acc: 0.687 - 3s 56ms/step - loss: 0.8818 - acc: 0.689 - 3s 55ms/step - loss: 0.8845 - acc: 0.688 - 4s 56ms/step - loss: 0.8861 - acc: 0.687 - 4s 56ms/step - loss: 0.8890 - acc: 0.686 - 4s 56ms/step - loss: 0.8932 - acc: 0.686 - 4s 56ms/step - loss: 0.8943 - acc: 0.685 - 4s 56ms/step - loss: 0.8965 - acc: 0.683 - 4s 56ms/step - loss: 0.8992 - acc: 0.682 - 4s 56ms/step - loss: 0.8978 - acc: 0.682 - 4s 56ms/step - loss: 0.8978 - acc: 0.683 - 4s 56ms/step - loss: 0.8984 - acc: 0.682 - 4s 56ms/step - loss: 0.9000 - acc: 0.683 - 4s 55ms/step - loss: 0.9019 - acc: 0.682 - 4s 55ms/step - loss: 0.9014 - acc: 0.682 - 4s 55ms/step - loss: 0.9013 - acc: 0.681 - 4s 55ms/step - loss: 0.9009 - acc: 0.681 - 4s 55ms/step - loss: 0.9009 - acc: 0.681 - 4s 55ms/step - loss: 0.8985 - acc: 0.681 - 4s 55ms/step - loss: 0.8999 - acc: 0.681 - 4s 55ms/step - loss: 0.8986 - acc: 0.681 - 4s 55ms/step - loss: 0.8987 - acc: 0.681 - 5s 55ms/step - loss: 0.8975 - acc: 0.682 - 5s 55ms/step - loss: 0.8977 - acc: 0.682 - 5s 55ms/step - loss: 0.8972 - acc: 0.681 - 5s 55ms/step - loss: 0.8962 - acc: 0.682 - 5s 55ms/step - loss: 0.8988 - acc: 0.681 - 5s 55ms/step - loss: 0.8991 - acc: 0.681 - 5s 55ms/step - loss: 0.8987 - acc: 0.681 - 5s 55ms/step - loss: 0.8996 - acc: 0.681 - 5s 55ms/step - loss: 0.9011 - acc: 0.680 - 5s 55ms/step - loss: 0.9008 - acc: 0.681 - 5s 55ms/step - loss: 0.9017 - acc: 0.681 - 5s 56ms/step - loss: 0.9028 - acc: 0.680 - 5s 56ms/step - loss: 0.9058 - acc: 0.680 - 5s 56ms/step - loss: 0.9080 - acc: 0.680 - 5s 56ms/step - loss: 0.9080 - acc: 0.680 - 5s 56ms/step - loss: 0.9125 - acc: 0.679 - 5s 56ms/step - loss: 0.9117 - acc: 0.680 - 6s 56ms/step - loss: 0.9110 - acc: 0.679 - 6s 56ms/step - loss: 0.9100 - acc: 0.680 - 6s 56ms/step - loss: 0.9108 - acc: 0.680 - 6s 56ms/step - loss: 0.9094 - acc: 0.680 - 6s 56ms/step - loss: 0.9065 - acc: 0.681 - 6s 56ms/step - loss: 0.9103 - acc: 0.680 - 6s 56ms/step - loss: 0.9110 - acc: 0.679 - 6s 56ms/step - loss: 0.9127 - acc: 0.678 - 6s 57ms/step - loss: 0.9151 - acc: 0.677 - 6s 57ms/step - loss: 0.9148 - acc: 0.678 - 6s 57ms/step - loss: 0.9162 - acc: 0.677 - 6s 57ms/step - loss: 0.9174 - acc: 0.677 - 6s 57ms/step - loss: 0.9167 - acc: 0.677 - 6s 57ms/step - loss: 0.9164 - acc: 0.677 - 6s 57ms/step - loss: 0.9171 - acc: 0.677 - 6s 57ms/step - loss: 0.9164 - acc: 0.677 - 7s 57ms/step - loss: 0.9163 - acc: 0.677 - 7s 57ms/step - loss: 0.9165 - acc: 0.677 - 7s 57ms/step - loss: 0.9166 - acc: 0.676 - 7s 57ms/step - loss: 0.9176 - acc: 0.676 - 7s 57ms/step - loss: 0.9183 - acc: 0.676 - 7s 57ms/step - loss: 0.9203 - acc: 0.675 - 7s 57ms/step - loss: 0.9210 - acc: 0.674 - 7s 57ms/step - loss: 0.9201 - acc: 0.674 - 7s 57ms/step - loss: 0.9192 - acc: 0.674 - 7s 57ms/step - loss: 0.9183 - acc: 0.675 - 7s 57ms/step - loss: 0.9174 - acc: 0.675 - 7s 57ms/step - loss: 0.9164 - acc: 0.676 - 7s 57ms/step - loss: 0.9180 - acc: 0.675 - 7s 57ms/step - loss: 0.9170 - acc: 0.675 - 7s 57ms/step - loss: 0.9170 - acc: 0.675 - 7s 57ms/step - loss: 0.9157 - acc: 0.676 - 7s 57ms/step - loss: 0.9144 - acc: 0.676 - 8s 57ms/step - loss: 0.9140 - acc: 0.676 - 8s 57ms/step - loss: 0.9138 - acc: 0.676 - 8s 57ms/step - loss: 0.9122 - acc: 0.677 - 8s 57ms/step - loss: 0.9114 - acc: 0.676 - 8s 57ms/step - loss: 0.9120 - acc: 0.676 - 8s 57ms/step - loss: 0.9125 - acc: 0.676 - 8s 57ms/step - loss: 0.9128 - acc: 0.676 - 8s 57ms/step - loss: 0.9142 - acc: 0.676 - 8s 57ms/step - loss: 0.9131 - acc: 0.676 - 8s 57ms/step - loss: 0.9132 - acc: 0.676 - 8s 57ms/step - loss: 0.9141 - acc: 0.676 - 8s 57ms/step - loss: 0.9138 - acc: 0.676 - 8s 57ms/step - loss: 0.9136 - acc: 0.676 - 8s 57ms/step - loss: 0.9127 - acc: 0.676 - 8s 57ms/step - loss: 0.9116 - acc: 0.677 - 8s 57ms/step - loss: 0.9109 - acc: 0.677 - 8s 57ms/step - loss: 0.9114 - acc: 0.677 - 8s 57ms/step - loss: 0.9112 - acc: 0.677 - 9s 57ms/step - loss: 0.9133 - acc: 0.677 - 9s 57ms/step - loss: 0.9126 - acc: 0.677 - 9s 57ms/step - loss: 0.9130 - acc: 0.677 - 9s 57ms/step - loss: 0.9130 - acc: 0.677 - 9s 57ms/step - loss: 0.9140 - acc: 0.677 - 9s 57ms/step - loss: 0.9145 - acc: 0.676 - 9s 57ms/step - loss: 0.9160 - acc: 0.676 - 9s 57ms/step - loss: 0.9170 - acc: 0.676 - 9s 57ms/step - loss: 0.9170 - acc: 0.6766"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9169710684733786, 0.6766]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.models.load_model('checkpoints/{}.hdf5'.format(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 32)          18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 101,866\n",
      "Trainable params: 101,418\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 203ms/step - loss: 0.7984 - acc: 0.71 - 0s 128ms/step - loss: 0.9168 - acc: 0.69 - 0s 103ms/step - loss: 0.9136 - acc: 0.67 - 0s 90ms/step - loss: 0.8498 - acc: 0.6953 - 0s 83ms/step - loss: 0.8475 - acc: 0.700 - 0s 78ms/step - loss: 0.8406 - acc: 0.703 - 1s 75ms/step - loss: 0.8446 - acc: 0.700 - 1s 72ms/step - loss: 0.8485 - acc: 0.701 - 1s 71ms/step - loss: 0.8446 - acc: 0.701 - 1s 69ms/step - loss: 0.8585 - acc: 0.700 - 1s 68ms/step - loss: 0.8619 - acc: 0.694 - 1s 67ms/step - loss: 0.8803 - acc: 0.692 - 1s 66ms/step - loss: 0.8682 - acc: 0.694 - 1s 65ms/step - loss: 0.8641 - acc: 0.695 - 1s 65ms/step - loss: 0.8579 - acc: 0.700 - 1s 64ms/step - loss: 0.8707 - acc: 0.696 - 1s 64ms/step - loss: 0.8670 - acc: 0.696 - 1s 63ms/step - loss: 0.8754 - acc: 0.691 - 1s 63ms/step - loss: 0.8785 - acc: 0.689 - 1s 63ms/step - loss: 0.8940 - acc: 0.683 - 1s 63ms/step - loss: 0.8814 - acc: 0.687 - 1s 63ms/step - loss: 0.8799 - acc: 0.691 - 1s 63ms/step - loss: 0.8787 - acc: 0.692 - 2s 63ms/step - loss: 0.8816 - acc: 0.691 - 2s 63ms/step - loss: 0.8743 - acc: 0.695 - 2s 63ms/step - loss: 0.8724 - acc: 0.695 - 2s 63ms/step - loss: 0.8700 - acc: 0.695 - 2s 62ms/step - loss: 0.8632 - acc: 0.696 - 2s 62ms/step - loss: 0.8661 - acc: 0.696 - 2s 62ms/step - loss: 0.8723 - acc: 0.695 - 2s 62ms/step - loss: 0.8732 - acc: 0.695 - 2s 62ms/step - loss: 0.8701 - acc: 0.696 - 2s 62ms/step - loss: 0.8749 - acc: 0.695 - 2s 62ms/step - loss: 0.8749 - acc: 0.695 - 2s 62ms/step - loss: 0.8736 - acc: 0.696 - 2s 62ms/step - loss: 0.8742 - acc: 0.694 - 2s 62ms/step - loss: 0.8702 - acc: 0.695 - 2s 62ms/step - loss: 0.8752 - acc: 0.692 - 2s 62ms/step - loss: 0.8750 - acc: 0.691 - 2s 61ms/step - loss: 0.8747 - acc: 0.691 - 3s 61ms/step - loss: 0.8776 - acc: 0.691 - 3s 61ms/step - loss: 0.8785 - acc: 0.690 - 3s 61ms/step - loss: 0.8812 - acc: 0.690 - 3s 61ms/step - loss: 0.8860 - acc: 0.689 - 3s 61ms/step - loss: 0.8781 - acc: 0.691 - 3s 61ms/step - loss: 0.8770 - acc: 0.690 - 3s 61ms/step - loss: 0.8771 - acc: 0.690 - 3s 61ms/step - loss: 0.8763 - acc: 0.692 - 3s 60ms/step - loss: 0.8780 - acc: 0.691 - 3s 60ms/step - loss: 0.8812 - acc: 0.689 - 3s 60ms/step - loss: 0.8836 - acc: 0.688 - 3s 60ms/step - loss: 0.8842 - acc: 0.687 - 3s 60ms/step - loss: 0.8888 - acc: 0.684 - 3s 60ms/step - loss: 0.8913 - acc: 0.683 - 3s 60ms/step - loss: 0.8873 - acc: 0.685 - 3s 60ms/step - loss: 0.8885 - acc: 0.685 - 3s 60ms/step - loss: 0.8858 - acc: 0.686 - 3s 60ms/step - loss: 0.8844 - acc: 0.687 - 4s 60ms/step - loss: 0.8829 - acc: 0.688 - 4s 60ms/step - loss: 0.8852 - acc: 0.687 - 4s 60ms/step - loss: 0.8818 - acc: 0.689 - 4s 60ms/step - loss: 0.8845 - acc: 0.688 - 4s 60ms/step - loss: 0.8861 - acc: 0.687 - 4s 60ms/step - loss: 0.8890 - acc: 0.686 - 4s 60ms/step - loss: 0.8932 - acc: 0.686 - 4s 60ms/step - loss: 0.8943 - acc: 0.685 - 4s 60ms/step - loss: 0.8965 - acc: 0.683 - 4s 60ms/step - loss: 0.8992 - acc: 0.682 - 4s 60ms/step - loss: 0.8978 - acc: 0.682 - 4s 60ms/step - loss: 0.8978 - acc: 0.683 - 4s 60ms/step - loss: 0.8984 - acc: 0.682 - 4s 60ms/step - loss: 0.9000 - acc: 0.683 - 4s 59ms/step - loss: 0.9019 - acc: 0.682 - 4s 59ms/step - loss: 0.9014 - acc: 0.682 - 4s 59ms/step - loss: 0.9013 - acc: 0.681 - 5s 59ms/step - loss: 0.9009 - acc: 0.681 - 5s 59ms/step - loss: 0.9009 - acc: 0.681 - 5s 59ms/step - loss: 0.8985 - acc: 0.681 - 5s 59ms/step - loss: 0.8999 - acc: 0.681 - 5s 59ms/step - loss: 0.8986 - acc: 0.681 - 5s 59ms/step - loss: 0.8987 - acc: 0.681 - 5s 59ms/step - loss: 0.8975 - acc: 0.682 - 5s 59ms/step - loss: 0.8977 - acc: 0.682 - 5s 59ms/step - loss: 0.8972 - acc: 0.681 - 5s 59ms/step - loss: 0.8962 - acc: 0.682 - 5s 59ms/step - loss: 0.8988 - acc: 0.681 - 5s 59ms/step - loss: 0.8991 - acc: 0.681 - 5s 59ms/step - loss: 0.8987 - acc: 0.681 - 5s 59ms/step - loss: 0.8996 - acc: 0.681 - 5s 59ms/step - loss: 0.9011 - acc: 0.680 - 5s 59ms/step - loss: 0.9008 - acc: 0.681 - 5s 59ms/step - loss: 0.9017 - acc: 0.681 - 5s 59ms/step - loss: 0.9028 - acc: 0.680 - 6s 59ms/step - loss: 0.9058 - acc: 0.680 - 6s 59ms/step - loss: 0.9080 - acc: 0.680 - 6s 59ms/step - loss: 0.9080 - acc: 0.680 - 6s 59ms/step - loss: 0.9125 - acc: 0.679 - 6s 59ms/step - loss: 0.9117 - acc: 0.680 - 6s 59ms/step - loss: 0.9110 - acc: 0.679 - 6s 59ms/step - loss: 0.9100 - acc: 0.680 - 6s 59ms/step - loss: 0.9108 - acc: 0.680 - 6s 59ms/step - loss: 0.9094 - acc: 0.680 - 6s 59ms/step - loss: 0.9065 - acc: 0.681 - 6s 59ms/step - loss: 0.9103 - acc: 0.680 - 6s 59ms/step - loss: 0.9110 - acc: 0.679 - 6s 59ms/step - loss: 0.9127 - acc: 0.678 - 6s 59ms/step - loss: 0.9151 - acc: 0.677 - 6s 59ms/step - loss: 0.9148 - acc: 0.678 - 6s 59ms/step - loss: 0.9162 - acc: 0.677 - 7s 59ms/step - loss: 0.9174 - acc: 0.677 - 7s 59ms/step - loss: 0.9167 - acc: 0.677 - 7s 59ms/step - loss: 0.9164 - acc: 0.677 - 7s 59ms/step - loss: 0.9171 - acc: 0.677 - 7s 59ms/step - loss: 0.9164 - acc: 0.677 - 7s 59ms/step - loss: 0.9163 - acc: 0.677 - 7s 59ms/step - loss: 0.9165 - acc: 0.677 - 7s 59ms/step - loss: 0.9166 - acc: 0.676 - 7s 59ms/step - loss: 0.9176 - acc: 0.676 - 7s 59ms/step - loss: 0.9183 - acc: 0.676 - 7s 59ms/step - loss: 0.9203 - acc: 0.675 - 7s 59ms/step - loss: 0.9210 - acc: 0.674 - 7s 59ms/step - loss: 0.9201 - acc: 0.674 - 7s 59ms/step - loss: 0.9192 - acc: 0.674 - 7s 59ms/step - loss: 0.9183 - acc: 0.675 - 7s 59ms/step - loss: 0.9174 - acc: 0.675 - 7s 59ms/step - loss: 0.9164 - acc: 0.676 - 7s 59ms/step - loss: 0.9180 - acc: 0.675 - 8s 59ms/step - loss: 0.9170 - acc: 0.675 - 8s 59ms/step - loss: 0.9170 - acc: 0.675 - 8s 59ms/step - loss: 0.9157 - acc: 0.676 - 8s 59ms/step - loss: 0.9144 - acc: 0.676 - 8s 59ms/step - loss: 0.9140 - acc: 0.676 - 8s 59ms/step - loss: 0.9138 - acc: 0.676 - 8s 59ms/step - loss: 0.9122 - acc: 0.677 - 8s 59ms/step - loss: 0.9114 - acc: 0.676 - 8s 59ms/step - loss: 0.9120 - acc: 0.676 - 8s 59ms/step - loss: 0.9125 - acc: 0.676 - 8s 59ms/step - loss: 0.9128 - acc: 0.676 - 8s 59ms/step - loss: 0.9142 - acc: 0.676 - 8s 59ms/step - loss: 0.9131 - acc: 0.676 - 8s 59ms/step - loss: 0.9132 - acc: 0.676 - 8s 59ms/step - loss: 0.9141 - acc: 0.676 - 8s 59ms/step - loss: 0.9138 - acc: 0.676 - 8s 59ms/step - loss: 0.9136 - acc: 0.676 - 9s 59ms/step - loss: 0.9127 - acc: 0.676 - 9s 59ms/step - loss: 0.9116 - acc: 0.677 - 9s 59ms/step - loss: 0.9109 - acc: 0.677 - 9s 59ms/step - loss: 0.9114 - acc: 0.677 - 9s 59ms/step - loss: 0.9112 - acc: 0.677 - 9s 59ms/step - loss: 0.9133 - acc: 0.677 - 9s 59ms/step - loss: 0.9126 - acc: 0.677 - 9s 59ms/step - loss: 0.9130 - acc: 0.677 - 9s 59ms/step - loss: 0.9130 - acc: 0.677 - 9s 59ms/step - loss: 0.9140 - acc: 0.677 - 9s 59ms/step - loss: 0.9145 - acc: 0.676 - 9s 59ms/step - loss: 0.9160 - acc: 0.676 - 9s 59ms/step - loss: 0.9170 - acc: 0.676 - 9s 59ms/step - loss: 0.9170 - acc: 0.6766"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9169710684733786, 0.6766]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/Unknown - 0s 237ms/step - loss: 1.1088 - acc: 0.59 - 0s 147ms/step - loss: 0.8076 - acc: 0.71 - 0s 117ms/step - loss: 0.8249 - acc: 0.70 - 0s 101ms/step - loss: 0.7909 - acc: 0.72 - 0s 92ms/step - loss: 0.8076 - acc: 0.7312 - 1s 86ms/step - loss: 0.8186 - acc: 0.716 - 1s 81ms/step - loss: 0.8124 - acc: 0.721 - 1s 78ms/step - loss: 0.8182 - acc: 0.709 - 1s 76ms/step - loss: 0.8082 - acc: 0.715 - 1s 74ms/step - loss: 0.8231 - acc: 0.710 - 1s 72ms/step - loss: 0.8483 - acc: 0.704 - 1s 71ms/step - loss: 0.8506 - acc: 0.701 - 1s 70ms/step - loss: 0.8382 - acc: 0.704 - 1s 69ms/step - loss: 0.8424 - acc: 0.704 - 1s 68ms/step - loss: 0.8624 - acc: 0.696 - 1s 67ms/step - loss: 0.8630 - acc: 0.699 - 1s 66ms/step - loss: 0.8585 - acc: 0.700 - 1s 66ms/step - loss: 0.8604 - acc: 0.699 - 1s 66ms/step - loss: 0.8540 - acc: 0.702 - 1s 65ms/step - loss: 0.8470 - acc: 0.703 - 1s 65ms/step - loss: 0.8474 - acc: 0.706 - 1s 65ms/step - loss: 0.8485 - acc: 0.707 - 1s 64ms/step - loss: 0.8437 - acc: 0.708 - 2s 64ms/step - loss: 0.8449 - acc: 0.708 - 2s 64ms/step - loss: 0.8374 - acc: 0.711 - 2s 64ms/step - loss: 0.8380 - acc: 0.713 - 2s 63ms/step - loss: 0.8466 - acc: 0.708 - 2s 63ms/step - loss: 0.8434 - acc: 0.709 - 2s 63ms/step - loss: 0.8379 - acc: 0.711 - 2s 63ms/step - loss: 0.8378 - acc: 0.712 - 2s 63ms/step - loss: 0.8398 - acc: 0.710 - 2s 63ms/step - loss: 0.8406 - acc: 0.709 - 2s 63ms/step - loss: 0.8470 - acc: 0.708 - 2s 63ms/step - loss: 0.8491 - acc: 0.708 - 2s 62ms/step - loss: 0.8539 - acc: 0.708 - 2s 62ms/step - loss: 0.8608 - acc: 0.707 - 2s 62ms/step - loss: 0.8714 - acc: 0.704 - 2s 62ms/step - loss: 0.8695 - acc: 0.705 - 2s 62ms/step - loss: 0.8641 - acc: 0.707 - 2s 62ms/step - loss: 0.8664 - acc: 0.707 - 3s 61ms/step - loss: 0.8667 - acc: 0.706 - 3s 61ms/step - loss: 0.8718 - acc: 0.704 - 3s 61ms/step - loss: 0.8719 - acc: 0.704 - 3s 61ms/step - loss: 0.8699 - acc: 0.706 - 3s 61ms/step - loss: 0.8698 - acc: 0.706 - 3s 61ms/step - loss: 0.8750 - acc: 0.704 - 3s 61ms/step - loss: 0.8779 - acc: 0.702 - 3s 61ms/step - loss: 0.8775 - acc: 0.702 - 3s 60ms/step - loss: 0.8788 - acc: 0.702 - 3s 60ms/step - loss: 0.8784 - acc: 0.702 - 3s 60ms/step - loss: 0.8799 - acc: 0.702 - 3s 60ms/step - loss: 0.8829 - acc: 0.700 - 3s 60ms/step - loss: 0.8834 - acc: 0.700 - 3s 60ms/step - loss: 0.8883 - acc: 0.697 - 3s 60ms/step - loss: 0.8871 - acc: 0.696 - 3s 60ms/step - loss: 0.8861 - acc: 0.697 - 3s 60ms/step - loss: 0.8833 - acc: 0.696 - 3s 60ms/step - loss: 0.8822 - acc: 0.697 - 4s 60ms/step - loss: 0.8833 - acc: 0.696 - 4s 60ms/step - loss: 0.8847 - acc: 0.695 - 4s 60ms/step - loss: 0.8864 - acc: 0.695 - 4s 60ms/step - loss: 0.8906 - acc: 0.694 - 4s 60ms/step - loss: 0.8876 - acc: 0.695 - 4s 60ms/step - loss: 0.8887 - acc: 0.695 - 4s 60ms/step - loss: 0.8860 - acc: 0.696 - 4s 60ms/step - loss: 0.8868 - acc: 0.696 - 4s 60ms/step - loss: 0.8883 - acc: 0.695 - 4s 60ms/step - loss: 0.8887 - acc: 0.694 - 4s 60ms/step - loss: 0.8907 - acc: 0.695 - 4s 60ms/step - loss: 0.8911 - acc: 0.694 - 4s 60ms/step - loss: 0.8913 - acc: 0.693 - 4s 60ms/step - loss: 0.8925 - acc: 0.691 - 4s 60ms/step - loss: 0.8947 - acc: 0.691 - 4s 59ms/step - loss: 0.9010 - acc: 0.690 - 4s 59ms/step - loss: 0.8998 - acc: 0.690 - 5s 59ms/step - loss: 0.8986 - acc: 0.690 - 5s 59ms/step - loss: 0.8990 - acc: 0.690 - 5s 59ms/step - loss: 0.8992 - acc: 0.690 - 5s 59ms/step - loss: 0.9007 - acc: 0.690 - 5s 59ms/step - loss: 0.8992 - acc: 0.691 - 5s 59ms/step - loss: 0.9010 - acc: 0.690 - 5s 59ms/step - loss: 0.9038 - acc: 0.688 - 5s 59ms/step - loss: 0.9031 - acc: 0.688 - 5s 59ms/step - loss: 0.9006 - acc: 0.689 - 5s 59ms/step - loss: 0.9046 - acc: 0.689 - 5s 59ms/step - loss: 0.9038 - acc: 0.689 - 5s 59ms/step - loss: 0.9025 - acc: 0.690 - 5s 59ms/step - loss: 0.9045 - acc: 0.689 - 5s 59ms/step - loss: 0.9046 - acc: 0.689 - 5s 59ms/step - loss: 0.9042 - acc: 0.689 - 5s 59ms/step - loss: 0.9045 - acc: 0.689 - 5s 59ms/step - loss: 0.9060 - acc: 0.688 - 5s 59ms/step - loss: 0.9092 - acc: 0.687 - 6s 59ms/step - loss: 0.9090 - acc: 0.687 - 6s 59ms/step - loss: 0.9109 - acc: 0.686 - 6s 59ms/step - loss: 0.9110 - acc: 0.685 - 6s 59ms/step - loss: 0.9110 - acc: 0.684 - 6s 59ms/step - loss: 0.9090 - acc: 0.684 - 6s 59ms/step - loss: 0.9083 - acc: 0.685 - 6s 59ms/step - loss: 0.9111 - acc: 0.684 - 6s 59ms/step - loss: 0.9110 - acc: 0.684 - 6s 59ms/step - loss: 0.9093 - acc: 0.685 - 6s 59ms/step - loss: 0.9092 - acc: 0.685 - 6s 59ms/step - loss: 0.9094 - acc: 0.685 - 6s 59ms/step - loss: 0.9089 - acc: 0.685 - 6s 59ms/step - loss: 0.9092 - acc: 0.685 - 6s 59ms/step - loss: 0.9100 - acc: 0.685 - 6s 59ms/step - loss: 0.9117 - acc: 0.684 - 6s 59ms/step - loss: 0.9104 - acc: 0.685 - 6s 59ms/step - loss: 0.9097 - acc: 0.685 - 7s 59ms/step - loss: 0.9090 - acc: 0.686 - 7s 59ms/step - loss: 0.9079 - acc: 0.686 - 7s 59ms/step - loss: 0.9093 - acc: 0.686 - 7s 59ms/step - loss: 0.9102 - acc: 0.686 - 7s 59ms/step - loss: 0.9107 - acc: 0.686 - 7s 59ms/step - loss: 0.9094 - acc: 0.686 - 7s 59ms/step - loss: 0.9081 - acc: 0.687 - 7s 59ms/step - loss: 0.9094 - acc: 0.686 - 7s 59ms/step - loss: 0.9101 - acc: 0.686 - 7s 59ms/step - loss: 0.9090 - acc: 0.687 - 7s 59ms/step - loss: 0.9092 - acc: 0.687 - 7s 59ms/step - loss: 0.9098 - acc: 0.686 - 7s 59ms/step - loss: 0.9100 - acc: 0.686 - 7s 59ms/step - loss: 0.9119 - acc: 0.685 - 7s 59ms/step - loss: 0.9115 - acc: 0.685 - 7s 59ms/step - loss: 0.9114 - acc: 0.685 - 7s 59ms/step - loss: 0.9099 - acc: 0.686 - 8s 59ms/step - loss: 0.9099 - acc: 0.685 - 8s 59ms/step - loss: 0.9105 - acc: 0.685 - 8s 59ms/step - loss: 0.9107 - acc: 0.685 - 8s 59ms/step - loss: 0.9097 - acc: 0.685 - 8s 59ms/step - loss: 0.9094 - acc: 0.686 - 8s 59ms/step - loss: 0.9097 - acc: 0.686 - 8s 59ms/step - loss: 0.9108 - acc: 0.685 - 8s 59ms/step - loss: 0.9107 - acc: 0.685 - 8s 59ms/step - loss: 0.9094 - acc: 0.685 - 8s 59ms/step - loss: 0.9090 - acc: 0.685 - 8s 59ms/step - loss: 0.9078 - acc: 0.686 - 8s 59ms/step - loss: 0.9081 - acc: 0.686 - 8s 59ms/step - loss: 0.9083 - acc: 0.686 - 8s 59ms/step - loss: 0.9081 - acc: 0.686 - 8s 59ms/step - loss: 0.9084 - acc: 0.686 - 8s 59ms/step - loss: 0.9094 - acc: 0.686 - 8s 59ms/step - loss: 0.9089 - acc: 0.686 - 8s 59ms/step - loss: 0.9083 - acc: 0.686 - 9s 59ms/step - loss: 0.9073 - acc: 0.687 - 9s 59ms/step - loss: 0.9068 - acc: 0.687 - 9s 59ms/step - loss: 0.9064 - acc: 0.687 - 9s 58ms/step - loss: 0.9078 - acc: 0.687 - 9s 58ms/step - loss: 0.9088 - acc: 0.686 - 9s 58ms/step - loss: 0.9100 - acc: 0.686 - 9s 58ms/step - loss: 0.9107 - acc: 0.686 - 9s 58ms/step - loss: 0.9100 - acc: 0.686 - 9s 58ms/step - loss: 0.9100 - acc: 0.686 - 9s 58ms/step - loss: 0.9103 - acc: 0.686 - 9s 58ms/step - loss: 0.9087 - acc: 0.687 - 9s 58ms/step - loss: 0.9085 - acc: 0.687 - 9s 58ms/step - loss: 0.9085 - acc: 0.6873"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9085101060047271, 0.6873]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
